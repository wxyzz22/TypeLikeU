{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "vPTLUhYcgJRe",
        "DpoIJo1HgLux",
        "vkw_gAFZgQnQ",
        "z562ietSgYt_",
        "EYlvFEjsgjVK"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VPRQRMR1fsKk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c90a568-865b-4700-d8cd-8ef603bd7712"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import re\n",
        "import os\n",
        "import datetime\n",
        "from pytz import timezone\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Dataset"
      ],
      "metadata": {
        "id": "KfrJn9OYgFtl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def processing_folder(folder_path, sample_size, train_size, dev_size):\n",
        "    os.chdir(folder_path)\n",
        "    files = sorted(os.listdir(folder_path), key=lambda x: int(re.findall(r'\\d+', x)[0]))\n",
        "    train_samples = []\n",
        "    dev_samples = []\n",
        "    test_samples = []\n",
        "    samples = []\n",
        "    cols = []\n",
        "    # text_count_id = 0\n",
        "    for i, path in enumerate(files):\n",
        "        if i >= sample_size:\n",
        "            break\n",
        "        with open(path, encoding='utf-8',           ##https://stackoverflow.com/questions/12468179/unicodedecodeerror-utf8-codec-cant-decode-byte-0x9c\n",
        "                 errors='ignore') as f:\n",
        "            lines = f.readlines()\n",
        "        ## extract column names (once)\n",
        "        if i == 0:\n",
        "            ls = lines[0].split('\\t')\n",
        "            if re.findall(r'\\w+|\\d+', ls[-1]):\n",
        "                ls[-1] = re.findall(r'\\w+|\\d+', ls[-1])[0]\n",
        "                cols = ls\n",
        "        ## extracting all samples from the current file\n",
        "        sample = []\n",
        "        curr_text_id = ''\n",
        "        curr_index = -1\n",
        "        for line in lines[1:]:\n",
        "            ls = line.split('\\t')\n",
        "            # ## debugdding block (to detect misaligned rows in files)\n",
        "            # if len(ls) != 9:\n",
        "            #     print(path, line)\n",
        "            if re.findall(r'\\w+|\\d+', ls[-1]):\n",
        "                ls[-1] = re.findall(r'\\w+|\\d+', ls[-1])[0]\n",
        "                if ls[1] != curr_text_id:\n",
        "                    curr_index = 0\n",
        "                    curr_text_id = ls[1]\n",
        "                    # text_count_id += 1\n",
        "                else:\n",
        "                    curr_index += 1\n",
        "                # ls.extend([curr_index, text_count_id])\n",
        "                ls.append(curr_index)\n",
        "                sample.append(ls)\n",
        "        ##  split the current data into train-test-sets\n",
        "        split_index_1 = int(train_size * len(sample))\n",
        "        split_index_2 = split_index_1 + int(dev_size * len(sample))\n",
        "        train_samples = train_samples + sample[:split_index_1]\n",
        "        dev_samples = dev_samples + sample[split_index_1:split_index_2]\n",
        "        test_samples = test_samples + sample[split_index_2:]\n",
        "        samples = samples + sample\n",
        "    ## forming dataframes: total data, train, test, dev\n",
        "    data = pd.DataFrame(samples)\n",
        "    train_data = pd.DataFrame(train_samples)\n",
        "    dev_data = pd.DataFrame(dev_samples)\n",
        "    test_data = pd.DataFrame(test_samples)\n",
        "    ## renaming columns\n",
        "    # cols = cols + ['INDEX'] + ['TEXT_COUNT_ID']\n",
        "    cols = cols + ['INDEX']\n",
        "    data.columns, train_data.columns, dev_data.columns, test_data.columns = cols, cols, cols, cols\n",
        "    ## construct onehot encoders from train data\n",
        "    train_data['K1'], train_data['K2'] = train_data['KEYCODE'], train_data['KEYCODE']\n",
        "    KEYCODE_enc = OneHotEncoder(handle_unknown='ignore').fit(train_data[['KEYCODE']])\n",
        "    K1_enc = OneHotEncoder(handle_unknown='ignore').fit(train_data[['K1']])\n",
        "    K2_enc = OneHotEncoder(handle_unknown='ignore').fit(train_data[['K2']])\n",
        "    K1_K2_enc = OneHotEncoder(handle_unknown='ignore').fit(train_data[['K1', 'K2']])\n",
        "    train_data = train_data.drop(columns=['K1', 'K2'])\n",
        "    return data, train_data, dev_data, test_data, KEYCODE_enc, K1_K2_enc, K1_enc, K2_enc"
      ],
      "metadata": {
        "id": "8MMCtBr2tBWU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## DEPRECATED: added dev_size to the newest function\n",
        "\n",
        "# def processing_folder(folder_path, sample_size, train_size):\n",
        "#     os.chdir(folder_path)\n",
        "#     files = sorted(os.listdir(folder_path), key=lambda x: int(re.findall(r'\\d+', x)[0]))\n",
        "#     train_samples = []\n",
        "#     test_samples = []\n",
        "#     samples = []\n",
        "#     cols = []\n",
        "#     # text_count_id = 0\n",
        "#     for i, path in enumerate(files):\n",
        "#         if i >= sample_size:\n",
        "#             break\n",
        "#         with open(path, encoding='utf-8',           ##https://stackoverflow.com/questions/12468179/unicodedecodeerror-utf8-codec-cant-decode-byte-0x9c\n",
        "#                  errors='ignore') as f:\n",
        "#             lines = f.readlines()\n",
        "#         ## extract column names (once)\n",
        "#         if i == 0:\n",
        "#             ls = lines[0].split('\\t')\n",
        "#             if re.findall(r'\\w+|\\d+', ls[-1]):\n",
        "#                 ls[-1] = re.findall(r'\\w+|\\d+', ls[-1])[0]\n",
        "#                 cols = ls\n",
        "#         ## extracting all samples from the current file\n",
        "#         sample = []\n",
        "#         curr_text_id = ''\n",
        "#         curr_index = -1\n",
        "#         for line in lines[1:]:\n",
        "#             ls = line.split('\\t')\n",
        "#             # ## debugdding block (to detect misaligned rows in files)\n",
        "#             # if len(ls) != 9:\n",
        "#             #     print(path, line)\n",
        "#             if re.findall(r'\\w+|\\d+', ls[-1]):\n",
        "#                 ls[-1] = re.findall(r'\\w+|\\d+', ls[-1])[0]\n",
        "#                 if ls[1] != curr_text_id:\n",
        "#                     curr_index = 0\n",
        "#                     curr_text_id = ls[1]\n",
        "#                     # text_count_id += 1\n",
        "#                 else:\n",
        "#                     curr_index += 1\n",
        "#                 # ls.extend([curr_index, text_count_id])\n",
        "#                 ls.append(curr_index)\n",
        "#                 sample.append(ls)\n",
        "#         ##  split the current data into train-test-sets\n",
        "#         split_index = int(train_size * len(sample))\n",
        "#         train_samples = train_samples + sample[:split_index]\n",
        "#         test_samples = test_samples + sample[split_index:]\n",
        "#         samples = samples + sample\n",
        "#     ## forming dataframes\n",
        "#     df_all = pd.DataFrame(samples)\n",
        "#     df_train = pd.DataFrame(train_samples)\n",
        "#     df_test = pd.DataFrame(test_samples)\n",
        "#     ## renaming columns\n",
        "#     # cols = cols + ['INDEX'] + ['TEXT_COUNT_ID']\n",
        "#     cols = cols + ['INDEX']\n",
        "#     df_all.columns, df_train.columns, df_test.columns = cols, cols, cols\n",
        "#     ## construct onehot encoders from train data\n",
        "#     df_train['K1'], df_train['K2'] = df_train['KEYCODE'], df_train['KEYCODE']\n",
        "#     KEYCODE_enc = OneHotEncoder(handle_unknown='ignore').fit(df_train[['KEYCODE']])\n",
        "#     K1_enc = OneHotEncoder(handle_unknown='ignore').fit(df_train[['K1']])\n",
        "#     K2_enc = OneHotEncoder(handle_unknown='ignore').fit(df_train[['K2']])\n",
        "#     K1_K2_enc = OneHotEncoder(handle_unknown='ignore').fit(df_train[['K1', 'K2']])\n",
        "#     df_train = df_train.drop(columns=['K1', 'K2'])\n",
        "#     return df_all, df_train, df_test, KEYCODE_enc, K1_K2_enc, K1_enc, K2_enc"
      ],
      "metadata": {
        "id": "ua3jqp9Uh9kY"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def processing_folder(folder_path, sample_size, train_size):\n",
        "#     os.chdir(folder_path)\n",
        "#     files = sorted(os.listdir(folder_path), key=lambda x: int(re.findall(r'\\d+', x)[0]))\n",
        "#     train_samples = []\n",
        "#     test_samples = []\n",
        "#     samples = []\n",
        "#     cols = []\n",
        "#     for i, path in enumerate(files):\n",
        "#         if i >= sample_size:\n",
        "#             break\n",
        "#         with open(path, encoding='utf-8',           ##https://stackoverflow.com/questions/12468179/unicodedecodeerror-utf8-codec-cant-decode-byte-0x9c\n",
        "#                  errors='ignore') as f:\n",
        "#             lines = f.readlines()\n",
        "#         ## extract column names (once)\n",
        "#         if i == 0:\n",
        "#             ls = lines[0].split('\\t')\n",
        "#             if re.findall(r'\\w+|\\d+', ls[-1]):\n",
        "#                 ls[-1] = re.findall(r'\\w+|\\d+', ls[-1])[0]\n",
        "#                 cols = ls\n",
        "#         ## extracting all samples from the current file\n",
        "#         sample = []\n",
        "#         curr_text_id = ''\n",
        "#         curr_index = -1\n",
        "#         for line in lines[1:]:\n",
        "#             ls = line.split('\\t')\n",
        "#             # if len(ls) != 9:\n",
        "#             #     print(path, line)\n",
        "#             if re.findall(r'\\w+|\\d+', ls[-1]):\n",
        "#                 ls[-1] = re.findall(r'\\w+|\\d+', ls[-1])[0]\n",
        "#                 if ls[1] != curr_text_id:\n",
        "#                     curr_index = 0\n",
        "#                     curr_text_id = ls[1]\n",
        "#                 else:\n",
        "#                     curr_index += 1\n",
        "#                 ls.append(curr_index)\n",
        "#                 sample.append(ls)\n",
        "#         ##  split the current data into train-test-sets\n",
        "#         split_index = int(train_size * len(sample))\n",
        "#         train_samples = train_samples + sample[:split_index]\n",
        "#         test_samples = test_samples + sample[split_index:]\n",
        "#         samples = samples + sample\n",
        "#     ## forming dataframes\n",
        "#     df_all = pd.DataFrame(samples)\n",
        "#     df_train = pd.DataFrame(train_samples)\n",
        "#     df_test = pd.DataFrame(test_samples)\n",
        "#     ## renaming columns\n",
        "#     cols = cols + ['INDEX']\n",
        "#     df_all.columns, df_train.columns, df_test.columns = cols, cols, cols\n",
        "#     ## construct onehot encoders\n",
        "#     df_all['K1'], df_all['K2'] = df_all['KEYCODE'], df_all['KEYCODE']\n",
        "#     uni_encoder = OneHotEncoder().fit(df_all[['KEYCODE']])\n",
        "#     di_encoder = OneHotEncoder().fit(df_all[['K1', 'K2']]) \n",
        "#     return df_all, df_train, df_test, uni_encoder, di_encoder"
      ],
      "metadata": {
        "id": "jWP8T0sZf5CI"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keyboard Layout Encoding"
      ],
      "metadata": {
        "id": "vPTLUhYcgJRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_qwerty_keyboard():\n",
        "    first_row = [27, 27, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 0, 0, 145, 126, 0, 0, 0, 0, 0]\n",
        "    space = [0] * 23\n",
        "    second_row = [192, 49, 50, 51, 52, 53, 54, 55, 56, 57, 48, 189, 187, 8, 0, 45, 36, 33, 0, 144, 111, 106, 109]\n",
        "    third_row = [9, 81, 87, 69, 82, 84, 89, 85, 73, 79, 80, 219, 221, 220, 0, 46, 35, 34, 0, 103, 104, 105, 107]\n",
        "    fourth_row = [20, 65, 83, 68, 70, 71, 72, 74, 75, 76, 186, 222, 13, 13, 0, 0, 0, 0, 0, 100, 101, 102, 107]\n",
        "    fifth_row = [16, 16, 90, 88, 67, 86, 66, 78, 77, 188, 190, 191, 16, 16, 0, 0, 38, 0, 0, 97, 98, 99, 13]\n",
        "    sixth_row = [17, 17, 191, 18, 32, 32, 32, 32, 32, 18, 92, 93, 17, 17, 0, 37, 40, 39, 0, 96, 96, 110, 13]\n",
        "    qwerty_keyboard = pd.DataFrame({'1st': first_row,\n",
        "                                    'space': space,\n",
        "                                    '2nd': second_row,\n",
        "                                    '3rd': third_row,\n",
        "                                    '4th': fourth_row,\n",
        "                                    '5th': fifth_row,\n",
        "                                    '6th': sixth_row}).transpose()\n",
        "    qwerty_keyboard.index = list(range(7))\n",
        "    return qwerty_keyboard\n",
        "\n",
        "class Keyboard:\n",
        "    def __init__(self, keyboard_df=get_qwerty_keyboard()):\n",
        "        self.keyboard = keyboard_df\n",
        "        self.keycode_pos = self.get_keycode_pos()\n",
        "    \n",
        "    def get_keycode_pos(self):\n",
        "        '''\n",
        "        Generates Python dictionary encoding the keyboard keycode positions, i.e.\n",
        "              - keys = javascript keycode\n",
        "              - values = [i, j] of the corresponding keycode position on the keyboard\n",
        "        Return: Python dict\n",
        "        '''\n",
        "        keyboard_dict = {}\n",
        "        for row in self.keyboard.index:\n",
        "            for col, entry in enumerate(self.keyboard.iloc[row, :]):\n",
        "                if entry in keyboard_dict:\n",
        "                    keyboard_dict[entry].append([row, col])\n",
        "                else:\n",
        "                    keyboard_dict[entry] = [[row, col]]\n",
        "        return keyboard_dict\n",
        "        \n",
        "    def keycode_distance(self, keycode1, keycode2):\n",
        "        '''\n",
        "        Given a pair of keycodes, return their relative distance on the keyboard\n",
        "        '''\n",
        "        keycode1 = int(keycode1)\n",
        "        keycode2 = int(keycode2)\n",
        "        def manhattan_dist(arr1, arr2):\n",
        "            return abs(arr1[0] - arr2[0]) + abs(arr1[1] - arr2[1])\n",
        "        distance = 30 ## any integer larger than 22+6\n",
        "        if keycode1 in self.keycode_pos and keycode2 in self.keycode_pos:\n",
        "            for arr1 in self.keycode_pos[keycode1]:\n",
        "                for arr2 in self.keycode_pos[keycode2]:\n",
        "                    curr_dist = manhattan_dist(arr1, arr2)\n",
        "                    if curr_dist < distance:\n",
        "                        distance = curr_dist\n",
        "            if distance < 5:\n",
        "                return distance\n",
        "        return 5\n",
        "    \n",
        "    def home_distance(self, keycode_list):\n",
        "        '''\n",
        "        Computes the AVERAGE distance of a list of keycodes to the home keys, where\n",
        "        In QWERTY keyboard, F and J are the home keys with keycodes 70 and 74 resp.\n",
        "        '''\n",
        "        sum = 0\n",
        "        for key in keycode_list:\n",
        "            key = int(key)\n",
        "            sum += min([self.keycode_distance(70, key), self.keycode_distance(74, key)])\n",
        "        return sum/len(keycode_list)\n",
        "    \n",
        "    def keyboard_dict(self):\n",
        "        return {'keycode': self.keycode_distance, 'home': self.home_distance}"
      ],
      "metadata": {
        "id": "Wfz4rEfkiDDK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_qwerty_keyboard():\n",
        "#     first_row = [27, 27, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 0, 0, 145, 126, 0, 0, 0, 0, 0]\n",
        "#     space = [0] * 23\n",
        "#     second_row = [192, 49, 50, 51, 52, 53, 54, 55, 56, 57, 48, 189, 187, 8, 0, 45, 36, 33, 0, 144, 111, 106, 109]\n",
        "#     third_row = [9, 81, 87, 69, 82, 84, 89, 85, 73, 79, 80, 219, 221, 220, 0, 46, 35, 34, 0, 103, 104, 105, 107]\n",
        "#     fourth_row = [20, 65, 83, 68, 70, 71, 72, 74, 75, 76, 186, 222, 13, 13, 0, 0, 0, 0, 0, 100, 101, 102, 107]\n",
        "#     fifth_row = [16, 16, 90, 88, 67, 86, 66, 78, 77, 188, 190, 191, 16, 16, 0, 0, 38, 0, 0, 97, 98, 99, 13]\n",
        "#     sixth_row = [17, 17, 191, 18, 32, 32, 32, 32, 32, 18, 92, 93, 17, 17, 0, 37, 40, 39, 0, 96, 96, 110, 13]\n",
        "#     qwerty_keyboard = pd.DataFrame({'1st': first_row,\n",
        "#                                     'space': space,\n",
        "#                                     '2nd': second_row,\n",
        "#                                     '3rd': third_row,\n",
        "#                                     '4th': fourth_row,\n",
        "#                                     '5th': fifth_row,\n",
        "#                                     '6th': sixth_row}).transpose()\n",
        "#     qwerty_keyboard.index = list(range(7))\n",
        "#     return qwerty_keyboard\n",
        "\n",
        "# class Keyboard:\n",
        "#     def __init__(self, keyboard_df):\n",
        "#         self.keyboard = keyboard_df\n",
        "#         self.keycode_pos = self.get_keycode_pos()\n",
        "    \n",
        "#     def get_keycode_pos(self):\n",
        "#         '''\n",
        "#         Generates Python dictionary encoding the keyboard keycode positions, i.e.\n",
        "#               - keys = javascript keycode\n",
        "#               - values = [i, j] of the corresponding keycode position on the keyboard\n",
        "#         Return: Python dict\n",
        "#         '''\n",
        "#         keyboard_dict = {}\n",
        "#         for row in self.keyboard.index:\n",
        "#             for col, entry in enumerate(self.keyboard.iloc[row, :]):\n",
        "#                 if entry in keyboard_dict:\n",
        "#                     keyboard_dict[entry].append([row, col])\n",
        "#                 else:\n",
        "#                     keyboard_dict[entry] = [[row, col]]\n",
        "#         return keyboard_dict\n",
        "        \n",
        "#     def keycode_distance(self, keycode1, keycode2):\n",
        "#         '''\n",
        "#         Given a pair of keycodes, return their relative distance on the keyboard\n",
        "#         '''\n",
        "#         def manhattan_dist(arr1, arr2):\n",
        "#             return abs(arr1[0] - arr2[0]) + abs(arr1[1] - arr2[1])\n",
        "#         distance = 30 ## any integer larger than 22+6\n",
        "#         if keycode1 in self.keycode_pos and keycode2 in self.keycode_pos:\n",
        "#             for arr1 in self.keycode_pos[keycode1]:\n",
        "#                 for arr2 in self.keycode_pos[keycode2]:\n",
        "#                     curr_dist = manhattan_dist(arr1, arr2)\n",
        "#                     if curr_dist < distance:\n",
        "#                         distance = curr_dist\n",
        "#             if distance < 5:\n",
        "#                 return distance\n",
        "#         return 5\n",
        "    \n",
        "#     def home_distance(self, keycode_list):\n",
        "#         '''\n",
        "#         Computes the AVERAGE distance of a list of keycodes to the home keys, where\n",
        "#         In QWERTY keyboard, F and J are the home keys with keycodes 70 and 74 resp.\n",
        "#         '''\n",
        "#         sum = 0\n",
        "#         for key in keycode_list:\n",
        "#             sum += min([self.keycode_distance(70, key), self.keycode_distance(74, key)])\n",
        "#         return sum/len(keycode_list)\n",
        "    \n",
        "#     def keyboard_dict(self):\n",
        "#         return {'keycode': self.keycode_distance, 'home': self.home_distance}"
      ],
      "metadata": {
        "id": "wyaVK_ESf-cf"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extractor"
      ],
      "metadata": {
        "id": "DpoIJo1HgLux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Extractor:\n",
        "    def __init__(self, sub_data, keyboard_dict=Keyboard().keyboard_dict(), latencies=['HL', 'PL', 'IL', 'RL']):\n",
        "        self.keyboard_dict = keyboard_dict\n",
        "        self.latencies = latencies\n",
        "\n",
        "        self.unigraph = self.unigraph_extractor(sub_data)\n",
        "        self.digraph = self.digraph_extractor(sub_data)\n",
        "    \n",
        "    def unigraph_extractor(self, df, user_str=True, keycode_str=True, drop_user=False):\n",
        "        '''\n",
        "        Generates unigraph related features and returns the dataframe\n",
        "        '''\n",
        "        df = df[['PARTICIPANT_ID', 'TEST_SECTION_ID', 'PRESS_TIME', 'RELEASE_TIME', 'KEYCODE', 'INDEX']]\n",
        "        df = df.astype('float64')\n",
        "        if user_str:\n",
        "            df['PARTICIPANT_ID'] = df['PARTICIPANT_ID'].astype('int64').astype(str)\n",
        "        df = df.rename(columns={'PARTICIPANT_ID': 'USER'})\n",
        "        if drop_user:\n",
        "            df = df.drop(columns=['USER'])\n",
        "        if keycode_str:\n",
        "            df['KEYCODE'] = df['KEYCODE'].astype('int64').astype(str)\n",
        "        ## construct new features\n",
        "        if 'HL' in self.latencies:\n",
        "            df['HL'] = df['RELEASE_TIME'] - df['PRESS_TIME']\n",
        "        if 'IL' in self.latencies:\n",
        "            df['IL'] = pd.concat([df['PRESS_TIME'][1:], pd.Series([0])], ignore_index=True) - df['RELEASE_TIME']\n",
        "        if 'RL' in self.latencies:\n",
        "            df['RL'] = pd.concat([df['RELEASE_TIME'][1:], pd.Series([0])], ignore_index=True) - df['RELEASE_TIME']\n",
        "        if 'PL' in self.latencies:\n",
        "            df['PL'] = pd.concat([df['PRESS_TIME'][1:], pd.Series([0])], ignore_index=True) - df['PRESS_TIME']\n",
        "        ## dropping rows where the NEXT row has INDEX==0 (indicating a transition to next sentence)\n",
        "        shift_txt = pd.concat([df['TEST_SECTION_ID'][1:], df['TEST_SECTION_ID'][-1:]], ignore_index=True) - df['TEST_SECTION_ID']\n",
        "        mask = shift_txt == 0\n",
        "        df = df.loc[mask]\n",
        "        ## cleaning irrelavant info\n",
        "        df = df.drop(columns=['PRESS_TIME', 'RELEASE_TIME', 'TEST_SECTION_ID'])\n",
        "        df = df.iloc[:-1, :]\n",
        "        return df\n",
        "    \n",
        "    def digraph_extractor(self, df, user_str=True, keycode_str=True, drop_user=False):\n",
        "        '''\n",
        "        Generates digraph related features and returns the dataframe\n",
        "        '''\n",
        "        df = df[['PARTICIPANT_ID', 'TEST_SECTION_ID', 'PRESS_TIME', 'RELEASE_TIME', 'KEYCODE', 'INDEX']]\n",
        "        df = df.astype('float64')\n",
        "        if user_str:\n",
        "            df['PARTICIPANT_ID'] = df['PARTICIPANT_ID'].astype('int64').astype(str)\n",
        "        df = df.rename(columns={'PARTICIPANT_ID': 'USER'})\n",
        "        if drop_user:\n",
        "            df = df.drop(columns=['USER'])\n",
        "        ## construct new features\n",
        "        df['K1'] = df['KEYCODE']\n",
        "        df['K2'] = pd.concat([df['KEYCODE'][1:], pd.Series([0])], ignore_index=True)\n",
        "        if keycode_str:\n",
        "            df['K1'] = df['K1'].astype('int64').astype(str)\n",
        "            df['K2'] = df['K2'].astype('int64').astype(str)\n",
        "        df['I1'] = df['INDEX']\n",
        "        df['I2'] = pd.concat([df['INDEX'][1:], pd.Series([0])], ignore_index=True)\n",
        "        if 'HL' in self.latencies:\n",
        "            df['HL1'] = df['RELEASE_TIME'] - df['PRESS_TIME']\n",
        "            df['HL2'] = pd.concat([df['HL1'][1:], pd.Series([0])], ignore_index=True)\n",
        "        if 'IL' in self.latencies:\n",
        "            df['IL'] = pd.concat([df['PRESS_TIME'][1:], pd.Series([0])], ignore_index=True) - df['RELEASE_TIME']\n",
        "        if 'RL' in self.latencies:\n",
        "            df['RL'] = pd.concat([df['RELEASE_TIME'][1:], pd.Series([0])], ignore_index=True) - df['RELEASE_TIME']\n",
        "        if 'PL' in self.latencies:\n",
        "            df['PL'] = pd.concat([df['PRESS_TIME'][1:], pd.Series([0])], ignore_index=True) - df['PRESS_TIME']\n",
        "        ## dropping instances where I2 is zero (indicating a transition to next sentence)\n",
        "        shift_txt = pd.concat([df['TEST_SECTION_ID'][1:], df['TEST_SECTION_ID'][-1:]], ignore_index=True) - df['TEST_SECTION_ID']\n",
        "        mask = shift_txt == 0\n",
        "        df = df.loc[mask]\n",
        "        ## cleaning irrelavant info\n",
        "        df = df.drop(columns=['PRESS_TIME', 'RELEASE_TIME', 'KEYCODE', 'INDEX', 'TEST_SECTION_ID'])\n",
        "        df = df.iloc[:-1, :]\n",
        "        return df  \n",
        "    \n",
        "    def unigraph_avg(self, avg_mode, data=None, drop_origin=True, rename_avg=True, round_avg=True):\n",
        "        '''\n",
        "        Generates unigraph with values replaced by average behavior\n",
        "        Input:\n",
        "            avg_mode: str, takes value in 'mean', 'median' (if not == 'mean', default to 'median')\n",
        "        Output:\n",
        "            unigraph dataframe with Average User data\n",
        "        '''\n",
        "        if data:\n",
        "            df = data.copy()\n",
        "        else:\n",
        "            df = self.unigraph.copy()\n",
        "        for XL in self.latencies:\n",
        "            df[XL+'_avg'] = df[XL]\n",
        "        for keycode in df['KEYCODE'].unique():\n",
        "            mask = df['KEYCODE'] == keycode\n",
        "            if avg_mode == 'mean':\n",
        "                avg_df = df.loc[mask, self.latencies].mean()\n",
        "            else:\n",
        "                avg_df = df.loc[mask, self.latencies].median()\n",
        "            for XL in self.latencies:\n",
        "                df.loc[mask, XL+'_avg'] = avg_df[XL]\n",
        "        if round_avg:\n",
        "            for XL in self.latencies:\n",
        "                df[XL+'_avg'] = round(df[XL+'_avg'])\n",
        "        if drop_origin:\n",
        "            df = df.drop(columns=self.latencies)\n",
        "        if drop_origin and rename_avg:\n",
        "            df = df.rename(columns=lambda name: name[:2] if '_avg' in name else name)\n",
        "        return df\n",
        "    \n",
        "    ## https://towardsdatascience.com/do-you-use-apply-in-pandas-there-is-a-600x-faster-way-d2497facfa66\n",
        "    def digraph_avg(self, avg_mode, data=None, drop_origin=True, rename_avg=True, round_avg=True):\n",
        "        '''\n",
        "        Generates digraph with values replaced by average behavior\n",
        "        Input:\n",
        "            avg_mode: str, takes value in 'mean', 'median' (if not == 'mean', default to 'median')\n",
        "        Output:\n",
        "            digraph dataframe with Average User data\n",
        "        '''\n",
        "        if data:\n",
        "            df = data.copy()\n",
        "        else:\n",
        "            df = self.digraph.copy()\n",
        "        df['K1_K2'] = df[['K1', 'K2']].apply(tuple, axis=1)\n",
        "        latencies = self.latencies.copy()\n",
        "        if 'HL' in latencies:\n",
        "            latencies.remove('HL')\n",
        "            latencies.insert(0, 'HL2')\n",
        "            latencies.insert(0, 'HL1')\n",
        "        for XL in latencies:\n",
        "            df[XL+'_avg'] = df[XL]\n",
        "        for pair in df['K1_K2'].unique():\n",
        "            mask = df['K1_K2'] == pair\n",
        "            if avg_mode == 'mean':\n",
        "                avg_df = df.loc[mask, latencies].mean()\n",
        "            else:\n",
        "                avg_df = df.loc[mask, latencies].median()\n",
        "            for XL in latencies:\n",
        "                df.loc[mask, XL+'_avg'] = avg_df[XL]\n",
        "        if round_avg:\n",
        "            for XL in latencies:\n",
        "                df[XL+'_avg'] = round(df[XL+'_avg'])\n",
        "        if drop_origin:\n",
        "            df = df.drop(columns=latencies+['K1_K2'])\n",
        "        if drop_origin and rename_avg:\n",
        "            df = df.rename(columns=lambda name: re.search(r'(.{2,3})(_avg)', name).group(1) if '_avg' in name else name)\n",
        "        return df\n",
        " \n",
        "    def unigraph_keyboard(self, avg_mode=None):\n",
        "        '''\n",
        "        Returns a unigraph dataframe with added keyboard layout features\n",
        "        '''\n",
        "        if avg_mode:\n",
        "            df = self.unigraph_avg(avg_mode)\n",
        "        else:\n",
        "            df = self.unigraph.copy()\n",
        "        home_dist = []\n",
        "        for row in df.index:\n",
        "            home_dist.append(self.keyboard_dict['home']([df['KEYCODE'][row]]))\n",
        "        df['HD'] = home_dist\n",
        "        # cols = list(df.columns[:-3]) + list(df.columns[-1:]) + list(df.columns[-3:-1])\n",
        "        num_cols = len(df.columns)\n",
        "        cols = list(df.columns[:num_cols-1-len(self.latencies)]) + list(df.columns[-1:]) + list(df.columns[-1-len(self.latencies):-1])\n",
        "        df = df[cols]\n",
        "        return df\n",
        "    \n",
        "    def digraph_keyboard(self, avg_mode=None):\n",
        "        '''\n",
        "        Returns a digraph dataframe with added keyboard layout features\n",
        "        '''\n",
        "        if avg_mode:\n",
        "            df = self.digraph_avg(avg_mode)\n",
        "        else:\n",
        "            df = self.digraph.copy()\n",
        "        keycode_dist = []\n",
        "        home_dist = []\n",
        "        for row in df.index:\n",
        "            keycode_dist.append(self.keyboard_dict['keycode'](df['K1'][row], df['K2'][row]))\n",
        "            home_dist.append(self.keyboard_dict['home']([df['K1'][row], df['K2'][row]]))\n",
        "        df['KD'] = keycode_dist\n",
        "        df['HD'] = home_dist\n",
        "        # cols = list(df.columns[:-5]) + list(df.columns[-2:]) + list(df.columns[-5:-2])\n",
        "        num_cols = len(df.columns)\n",
        "        cols = list(df.columns[:num_cols-2-(len(self.latencies)+1)]) + list(df.columns[-2:]) + list(df.columns[-2-(len(self.latencies)+1):-2])\n",
        "        df = df[cols]\n",
        "        return df\n",
        "    \n",
        "    def IQR_filter(self, data, fold):\n",
        "        Q3 = data.quantile(.75)\n",
        "        Q1 = data.quantile(.25)\n",
        "        IQR = Q3 - Q1\n",
        "        max = Q3 + fold * IQR\n",
        "        min = Q1 - fold * IQR\n",
        "        return min, max\n",
        "\n",
        "    def ABS_filter(self, data, bounds):\n",
        "        num_bottom, num_top = bounds\n",
        "        min = data.sort_values()[:num_bottom+1].values[-1]\n",
        "        max = data.sort_values(ascending=False)[:num_top+1].values[-1]\n",
        "        return min, max\n",
        "    \n",
        "    def unigraph_filtered(self, avg_mode, encode_keyboard, filter, bounds_dict):\n",
        "        '''\n",
        "        Input:\n",
        "            avg_mode: str, takes value in ['median', 'mean', None(default)]\n",
        "            encode_keyboard: boolean\n",
        "            filter: str, takes value in ['ABS'(default), 'IQR']\n",
        "            bounds_dict: a python dictionary with keys=latencies, \n",
        "                                                  values=needed params\n",
        "                      ==> for IQR: values = folds (i.e. scaling IQR by fold*IQR)\n",
        "                      ==> for ABS: values = [num_bottoms, num_tops]\n",
        "        '''\n",
        "        filter_latencies = list(bounds_dict.keys())\n",
        "        if encode_keyboard:\n",
        "            df = self.unigraph_keyboard(avg_mode)\n",
        "        elif avg_mode:\n",
        "            df = self.unigraph_avg(avg_mode)\n",
        "        else:\n",
        "            df = self.unigraph.copy()\n",
        "        for latency in filter_latencies:\n",
        "            for user in df['USER'].unique():\n",
        "                mask_user = df['USER'] == user\n",
        "                mask_nonuser = df['USER'] != user\n",
        "                subdf = df.loc[mask_user, latency]\n",
        "                if filter == 'IQR':\n",
        "                    min, max = self.IQR_filter(subdf, bounds_dict[latency])\n",
        "                else:\n",
        "                    min, max = self.ABS_filter(subdf, bounds_dict[latency])\n",
        "                mask_max = df[latency] <= max\n",
        "                mask_min = df[latency] >= min\n",
        "                df = df.loc[mask_user & mask_max & mask_min | mask_nonuser]\n",
        "        return df\n",
        "    \n",
        "    def digraph_filtered(self, avg_mode, encode_keyboard, filter, bounds_dict):\n",
        "        '''\n",
        "        Input:\n",
        "            avg_mode: str, takes value in ['median', 'mean', None(default)]\n",
        "            encode_keyboard: boolean\n",
        "            filter: str, takes value in ['ABS'(default), 'IQR']\n",
        "            bounds_dict: a python dictionary with keys=latencies, \n",
        "                                                  values=needed params\n",
        "                      ==> for IQR: values = folds (i.e. scaling IQR by fold*IQR)\n",
        "                      ==> for ABS: values = [num_bottoms, num_tops]\n",
        "        '''\n",
        "        filter_latencies = list(bounds_dict.keys())\n",
        "        if encode_keyboard:\n",
        "            df = self.digraph_keyboard(avg_mode)\n",
        "        elif avg_mode:\n",
        "            df = self.digraph_avg(avg_mode)\n",
        "        else:\n",
        "            df = self.digraph.copy()\n",
        "        for latency in filter_latencies:\n",
        "            for user in df['USER'].unique():\n",
        "                mask_user = df['USER'] == user\n",
        "                mask_nonuser = df['USER'] != user\n",
        "                subdf = df.loc[mask_user, latency]\n",
        "                if filter == 'IQR':\n",
        "                    min, max = self.IQR_filter(subdf, bounds_dict[latency])\n",
        "                else:\n",
        "                    min, max = self.ABS_filter(subdf, bounds_dict[latency])\n",
        "                mask_max = df[latency] <= max\n",
        "                mask_min = df[latency] >= min\n",
        "                df = df.loc[mask_user & mask_max & mask_min | mask_nonuser]\n",
        "        return df"
      ],
      "metadata": {
        "id": "NXxo8q8lyc0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## DEPRECATED: added docstring and reordered the functions into unigraph-digraph groups order\n",
        "\n",
        "# class Extractor:\n",
        "#     def __init__(self, sub_data, keyboard_dict=Keyboard().keyboard_dict(), latencies=['HL', 'PL', 'IL', 'RL']):\n",
        "#         self.keyboard_dict = keyboard_dict\n",
        "#         self.latencies = latencies\n",
        "\n",
        "#         self.unigraph = self.unigraph_extractor(sub_data)\n",
        "#         self.digraph = self.digraph_extractor(sub_data)\n",
        "    \n",
        "#     def unigraph_extractor(self, df, user_str=True, keycode_str=True, drop_user=False):\n",
        "#         df = df[['PARTICIPANT_ID', 'TEST_SECTION_ID', 'PRESS_TIME', 'RELEASE_TIME', 'KEYCODE', 'INDEX']]\n",
        "#         df = df.astype('float64')\n",
        "#         if user_str:\n",
        "#             df['PARTICIPANT_ID'] = df['PARTICIPANT_ID'].astype('int64').astype(str)\n",
        "#         df = df.rename(columns={'PARTICIPANT_ID': 'USER'})\n",
        "#         if drop_user:\n",
        "#             df = df.drop(columns=['USER'])\n",
        "#         if keycode_str:\n",
        "#             df['KEYCODE'] = df['KEYCODE'].astype('int64').astype(str)\n",
        "#         ## construct new features\n",
        "#         if 'HL' in self.latencies:\n",
        "#             df['HL'] = df['RELEASE_TIME'] - df['PRESS_TIME']\n",
        "#         if 'IL' in self.latencies:\n",
        "#             df['IL'] = pd.concat([df['PRESS_TIME'][1:], pd.Series([0])], ignore_index=True) - df['RELEASE_TIME']\n",
        "#         if 'RL' in self.latencies:\n",
        "#             df['RL'] = pd.concat([df['RELEASE_TIME'][1:], pd.Series([0])], ignore_index=True) - df['RELEASE_TIME']\n",
        "#         if 'PL' in self.latencies:\n",
        "#             df['PL'] = pd.concat([df['PRESS_TIME'][1:], pd.Series([0])], ignore_index=True) - df['PRESS_TIME']\n",
        "#         ## dropping rows where the NEXT row has INDEX==0 (indicating a transition to next sentence)\n",
        "#         shift_txt = pd.concat([df['TEST_SECTION_ID'][1:], df['TEST_SECTION_ID'][-1:]], ignore_index=True) - df['TEST_SECTION_ID']\n",
        "#         mask = shift_txt == 0\n",
        "#         df = df.loc[mask]\n",
        "#         ## cleaning irrelavant info\n",
        "#         df = df.drop(columns=['PRESS_TIME', 'RELEASE_TIME', 'TEST_SECTION_ID'])\n",
        "#         df = df.iloc[:-1, :]\n",
        "#         return df\n",
        "    \n",
        "#     def digraph_extractor(self, df, user_str=True, keycode_str=True, drop_user=False):\n",
        "#         df = df[['PARTICIPANT_ID', 'TEST_SECTION_ID', 'PRESS_TIME', 'RELEASE_TIME', 'KEYCODE', 'INDEX']]\n",
        "#         df = df.astype('float64')\n",
        "#         if user_str:\n",
        "#             df['PARTICIPANT_ID'] = df['PARTICIPANT_ID'].astype('int64').astype(str)\n",
        "#         df = df.rename(columns={'PARTICIPANT_ID': 'USER'})\n",
        "#         if drop_user:\n",
        "#             df = df.drop(columns=['USER'])\n",
        "#         ## construct new features\n",
        "#         df['K1'] = df['KEYCODE']\n",
        "#         df['K2'] = pd.concat([df['KEYCODE'][1:], pd.Series([0])], ignore_index=True)\n",
        "#         if keycode_str:\n",
        "#             df['K1'] = df['K1'].astype('int64').astype(str)\n",
        "#             df['K2'] = df['K2'].astype('int64').astype(str)\n",
        "#         df['I1'] = df['INDEX']\n",
        "#         df['I2'] = pd.concat([df['INDEX'][1:], pd.Series([0])], ignore_index=True)\n",
        "#         if 'HL' in self.latencies:\n",
        "#             df['HL1'] = df['RELEASE_TIME'] - df['PRESS_TIME']\n",
        "#             df['HL2'] = pd.concat([df['HL1'][1:], pd.Series([0])], ignore_index=True)\n",
        "#         if 'IL' in self.latencies:\n",
        "#             df['IL'] = pd.concat([df['PRESS_TIME'][1:], pd.Series([0])], ignore_index=True) - df['RELEASE_TIME']\n",
        "#         if 'RL' in self.latencies:\n",
        "#             df['RL'] = pd.concat([df['RELEASE_TIME'][1:], pd.Series([0])], ignore_index=True) - df['RELEASE_TIME']\n",
        "#         if 'PL' in self.latencies:\n",
        "#             df['PL'] = pd.concat([df['PRESS_TIME'][1:], pd.Series([0])], ignore_index=True) - df['PRESS_TIME']\n",
        "#         ## dropping instances where I2 is zero (indicating a transition to next sentence)\n",
        "#         shift_txt = pd.concat([df['TEST_SECTION_ID'][1:], df['TEST_SECTION_ID'][-1:]], ignore_index=True) - df['TEST_SECTION_ID']\n",
        "#         mask = shift_txt == 0\n",
        "#         df = df.loc[mask]\n",
        "#         ## cleaning irrelavant info\n",
        "#         df = df.drop(columns=['PRESS_TIME', 'RELEASE_TIME', 'KEYCODE', 'INDEX', 'TEST_SECTION_ID'])\n",
        "#         df = df.iloc[:-1, :]\n",
        "#         return df\n",
        "    \n",
        "#     ## https://towardsdatascience.com/do-you-use-apply-in-pandas-there-is-a-600x-faster-way-d2497facfa66\n",
        "#     def digraph_avg(self, avg_mode, data=None, drop_origin=True, rename_avg=True, round_avg=True):\n",
        "#         if data:\n",
        "#             df = data.copy()\n",
        "#         else:\n",
        "#             df = self.digraph.copy()\n",
        "#         df['K1_K2'] = df[['K1', 'K2']].apply(tuple, axis=1)\n",
        "#         latencies = self.latencies.copy()\n",
        "#         if 'HL' in latencies:\n",
        "#             latencies.remove('HL')\n",
        "#             latencies.insert(0, 'HL2')\n",
        "#             latencies.insert(0, 'HL1')\n",
        "#         for XL in latencies:\n",
        "#             df[XL+'_avg'] = df[XL]\n",
        "#         for pair in df['K1_K2'].unique():\n",
        "#             mask = df['K1_K2'] == pair\n",
        "#             if avg_mode == 'mean':\n",
        "#                 avg_df = df.loc[mask, latencies].mean()\n",
        "#             else:\n",
        "#                 avg_df = df.loc[mask, latencies].median()\n",
        "#             for XL in latencies:\n",
        "#                 df.loc[mask, XL+'_avg'] = avg_df[XL]\n",
        "#         if round_avg:\n",
        "#             for XL in latencies:\n",
        "#                 df[XL+'_avg'] = round(df[XL+'_avg'])\n",
        "#         if drop_origin:\n",
        "#             df = df.drop(columns=latencies+['K1_K2'])\n",
        "#         if drop_origin and rename_avg:\n",
        "#             df = df.rename(columns=lambda name: re.search(r'(.{2,3})(_avg)', name).group(1) if '_avg' in name else name)\n",
        "#         return df\n",
        "\n",
        "#     def unigraph_avg(self, avg_mode, data=None, drop_origin=True, rename_avg=True, round_avg=True):\n",
        "#         if data:\n",
        "#             df = data.copy()\n",
        "#         else:\n",
        "#             df = self.unigraph.copy()\n",
        "#         for XL in self.latencies:\n",
        "#             df[XL+'_avg'] = df[XL]\n",
        "#         for keycode in df['KEYCODE'].unique():\n",
        "#             mask = df['KEYCODE'] == keycode\n",
        "#             if avg_mode == 'mean':\n",
        "#                 avg_df = df.loc[mask, self.latencies].mean()\n",
        "#             else:\n",
        "#                 avg_df = df.loc[mask, self.latencies].median()\n",
        "#             for XL in self.latencies:\n",
        "#                 df.loc[mask, XL+'_avg'] = avg_df[XL]\n",
        "#         if round_avg:\n",
        "#             for XL in self.latencies:\n",
        "#                 df[XL+'_avg'] = round(df[XL+'_avg'])\n",
        "#         if drop_origin:\n",
        "#             df = df.drop(columns=self.latencies)\n",
        "#         if drop_origin and rename_avg:\n",
        "#             df = df.rename(columns=lambda name: name[:2] if '_avg' in name else name)\n",
        "#         return df\n",
        "    \n",
        "#     def digraph_keyboard(self, avg_mode=None):\n",
        "#         if avg_mode:\n",
        "#             df = self.digraph_avg(avg_mode)\n",
        "#         else:\n",
        "#             df = self.digraph.copy()\n",
        "#         keycode_dist = []\n",
        "#         home_dist = []\n",
        "#         for row in df.index:\n",
        "#             keycode_dist.append(self.keyboard_dict['keycode'](df['K1'][row], df['K2'][row]))\n",
        "#             home_dist.append(self.keyboard_dict['home']([df['K1'][row], df['K2'][row]]))\n",
        "#         df['KD'] = keycode_dist\n",
        "#         df['HD'] = home_dist\n",
        "#         # cols = list(df.columns[:-5]) + list(df.columns[-2:]) + list(df.columns[-5:-2])\n",
        "#         num_cols = len(df.columns)\n",
        "#         cols = list(df.columns[:num_cols-2-(len(self.latencies)+1)]) + list(df.columns[-2:]) + list(df.columns[-2-(len(self.latencies)+1):-2])\n",
        "#         df = df[cols]\n",
        "#         return df\n",
        "    \n",
        "#     def unigraph_keyboard(self, avg_mode=None):\n",
        "#         if avg_mode:\n",
        "#             df = self.unigraph_avg(avg_mode)\n",
        "#         else:\n",
        "#             df = self.unigraph.copy()\n",
        "#         home_dist = []\n",
        "#         for row in df.index:\n",
        "#             home_dist.append(self.keyboard_dict['home']([df['KEYCODE'][row]]))\n",
        "#         df['HD'] = home_dist\n",
        "#         # cols = list(df.columns[:-3]) + list(df.columns[-1:]) + list(df.columns[-3:-1])\n",
        "#         num_cols = len(df.columns)\n",
        "#         cols = list(df.columns[:num_cols-1-len(self.latencies)]) + list(df.columns[-1:]) + list(df.columns[-1-len(self.latencies):-1])\n",
        "#         df = df[cols]\n",
        "#         return df\n",
        "    \n",
        "#     def IQR_filter(self, data, fold):\n",
        "#         Q3 = data.quantile(.75)\n",
        "#         Q1 = data.quantile(.25)\n",
        "#         IQR = Q3 - Q1\n",
        "#         max = Q3 + fold * IQR\n",
        "#         min = Q1 - fold * IQR\n",
        "#         return min, max\n",
        "\n",
        "#     def ABS_filter(self, data, bounds):\n",
        "#         num_bottom, num_top = bounds\n",
        "#         min = data.sort_values()[:num_bottom+1].values[-1]\n",
        "#         max = data.sort_values(ascending=False)[:num_top+1].values[-1]\n",
        "#         return min, max\n",
        "    \n",
        "#     def unigraph_filtered(self, avg_mode, encode_keyboard, filter, bounds_dict):\n",
        "#         '''\n",
        "#         Input:\n",
        "#             avg_mode: str, takes value in ['median', 'mean', None(default)]\n",
        "#             encode_keyboard: boolean\n",
        "#             filter: str, takes value in ['ABS'(default), 'IQR']\n",
        "#             bounds_dict: a python dictionary with keys=latencies, \n",
        "#                                                   values=needed params\n",
        "#                       ==> for IQR: values = folds (i.e. scaling IQR by fold*IQR)\n",
        "#                       ==> for ABS: values = [num_bottoms, num_tops]\n",
        "#         '''\n",
        "#         filter_latencies = list(bounds_dict.keys())\n",
        "#         if encode_keyboard:\n",
        "#             df = self.unigraph_keyboard(avg_mode)\n",
        "#         elif avg_mode:\n",
        "#             df = self.unigraph_avg(avg_mode)\n",
        "#         else:\n",
        "#             df = self.unigraph.copy()\n",
        "#         for latency in filter_latencies:\n",
        "#             for user in df['USER'].unique():\n",
        "#                 mask_user = df['USER'] == user\n",
        "#                 mask_nonuser = df['USER'] != user\n",
        "#                 subdf = df.loc[mask_user, latency]\n",
        "#                 if filter == 'IQR':\n",
        "#                     min, max = self.IQR_filter(subdf, bounds_dict[latency])\n",
        "#                 else:\n",
        "#                     min, max = self.ABS_filter(subdf, bounds_dict[latency])\n",
        "#                 mask_max = df[latency] <= max\n",
        "#                 mask_min = df[latency] >= min\n",
        "#                 df = df.loc[mask_user & mask_max & mask_min | mask_nonuser]\n",
        "#         return df\n",
        "    \n",
        "#     def digraph_filtered(self, avg_mode, encode_keyboard, filter, bounds_dict):\n",
        "#         '''\n",
        "#         Input:\n",
        "#             avg_mode: str, takes value in ['median', 'mean', None(default)]\n",
        "#             encode_keyboard: boolean\n",
        "#             filter: str, takes value in ['ABS'(default), 'IQR']\n",
        "#             bounds_dict: a python dictionary with keys=latencies, \n",
        "#                                                   values=needed params\n",
        "#                       ==> for IQR: values = folds (i.e. scaling IQR by fold*IQR)\n",
        "#                       ==> for ABS: values = [num_bottoms, num_tops]\n",
        "#         '''\n",
        "#         filter_latencies = list(bounds_dict.keys())\n",
        "#         if encode_keyboard:\n",
        "#             df = self.digraph_keyboard(avg_mode)\n",
        "#         elif avg_mode:\n",
        "#             df = self.digraph_avg(avg_mode)\n",
        "#         else:\n",
        "#             df = self.digraph.copy()\n",
        "#         for latency in filter_latencies:\n",
        "#             for user in df['USER'].unique():\n",
        "#                 mask_user = df['USER'] == user\n",
        "#                 mask_nonuser = df['USER'] != user\n",
        "#                 subdf = df.loc[mask_user, latency]\n",
        "#                 if filter == 'IQR':\n",
        "#                     min, max = self.IQR_filter(subdf, bounds_dict[latency])\n",
        "#                 else:\n",
        "#                     min, max = self.ABS_filter(subdf, bounds_dict[latency])\n",
        "#                 mask_max = df[latency] <= max\n",
        "#                 mask_min = df[latency] >= min\n",
        "#                 df = df.loc[mask_user & mask_max & mask_min | mask_nonuser]\n",
        "#         return df"
      ],
      "metadata": {
        "id": "kGK7qrNyiI8Q"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Deprecated : delete is_testset variable, deal with preprocessing more uniformly\n",
        "\n",
        "# class Extractors:\n",
        "#     def __init__(self, sub_data, keyboard_dict, latencies, is_testset):\n",
        "#         self.keyboard_dict = keyboard_dict\n",
        "#         self.latencies = latencies\n",
        "#         self.is_testset = is_testset\n",
        "\n",
        "#         self.unigraph = self.unigraph_extractor(sub_data)\n",
        "#         self.digraph = self.digraph_extractor(sub_data)\n",
        "    \n",
        "#     def unigraph_extractor(self, df, user_int=True, keycode_str=True, drop_user=False):\n",
        "#         df = df[['PARTICIPANT_ID', 'TEST_SECTION_ID', 'PRESS_TIME', 'RELEASE_TIME', 'KEYCODE', 'INDEX']]\n",
        "#         df = df.astype('float64')\n",
        "#         if user_int:\n",
        "#             df['PARTICIPANT_ID'] = df['PARTICIPANT_ID'].astype('int64')\n",
        "#         df = df.rename(columns={'PARTICIPANT_ID': 'USER'})\n",
        "#         if drop_user:\n",
        "#             df = df.drop(columns=['USER'])\n",
        "#         if keycode_str:\n",
        "#             df['KEYCODE'] = df['KEYCODE'].astype('int64').astype(str)\n",
        "#         ## construct new features\n",
        "#         if 'HL' in self.latencies:\n",
        "#             df['HL'] = df['RELEASE_TIME'] - df['PRESS_TIME']\n",
        "#         if 'IL' in self.latencies:\n",
        "#             df['IL'] = pd.concat([df['PRESS_TIME'][1:], pd.Series([0])], ignore_index=True) - df['RELEASE_TIME']\n",
        "#         if 'RL' in self.latencies:\n",
        "#             df['RL'] = pd.concat([df['RELEASE_TIME'][1:], pd.Series([0])], ignore_index=True) - df['RELEASE_TIME']\n",
        "#         if 'PL' in self.latencies:\n",
        "#             df['PL'] = pd.concat([df['PRESS_TIME'][1:], pd.Series([0])], ignore_index=True) - df['PRESS_TIME']\n",
        "#         ## dropping rows where the NEXT row has INDEX==0 (indicating a transition to next sentence)\n",
        "#         if self.is_testset:\n",
        "#             shift_txt = pd.concat([df['TEST_SECTION_ID'][1:], pd.Series([0])], ignore_index=True) - df['TEST_SECTION_ID']\n",
        "#             mask = list((shift_txt == 0)[:-1]) + [True]\n",
        "#         else:\n",
        "#             mask =  list((df['INDEX'] != 0)[1:]) + [False]\n",
        "#         df = df.loc[mask]\n",
        "#         ## cleaning irrelavant info\n",
        "#         df = df.drop(columns=['PRESS_TIME', 'RELEASE_TIME', 'TEST_SECTION_ID'])\n",
        "#         df = df.iloc[:-1, :]\n",
        "#         return df\n",
        "    \n",
        "#     def digraph_extractor(self, df, user_int=True, keycode_str=True, drop_user=False):\n",
        "#         df = df[['PARTICIPANT_ID', 'TEST_SECTION_ID', 'PRESS_TIME', 'RELEASE_TIME', 'KEYCODE', 'INDEX']]\n",
        "#         df = df.astype('float64')\n",
        "#         if user_int:\n",
        "#             df['PARTICIPANT_ID'] = df['PARTICIPANT_ID'].astype('int64')\n",
        "#         df = df.rename(columns={'PARTICIPANT_ID': 'USER'})\n",
        "#         if drop_user:\n",
        "#             df = df.drop(columns=['USER'])\n",
        "#         ## construct new features\n",
        "#         df['K1'] = df['KEYCODE']\n",
        "#         df['K2'] = pd.concat([df['KEYCODE'][1:], pd.Series([0])], ignore_index=True)\n",
        "#         if keycode_str:\n",
        "#             df['K1'] = df['K1'].astype('int64').astype(str)\n",
        "#             df['K2'] = df['K2'].astype('int64').astype(str)\n",
        "#         df['I1'] = df['INDEX']\n",
        "#         df['I2'] = pd.concat([df['INDEX'][1:], pd.Series([0])], ignore_index=True)\n",
        "#         if 'HL' in self.latencies:\n",
        "#             df['HL1'] = df['RELEASE_TIME'] - df['PRESS_TIME']\n",
        "#             df['HL2'] = pd.concat([df['HL1'][1:], pd.Series([0])], ignore_index=True)\n",
        "#         if 'IL' in self.latencies:\n",
        "#             df['IL'] = pd.concat([df['PRESS_TIME'][1:], pd.Series([0])], ignore_index=True) - df['RELEASE_TIME']\n",
        "#         if 'RL' in self.latencies:\n",
        "#             df['RL'] = pd.concat([df['RELEASE_TIME'][1:], pd.Series([0])], ignore_index=True) - df['RELEASE_TIME']\n",
        "#         if 'PL' in self.latencies:\n",
        "#             df['PL'] = pd.concat([df['PRESS_TIME'][1:], pd.Series([0])], ignore_index=True) - df['PRESS_TIME']\n",
        "#         ## dropping instances where I2 is zero (indicating a transition to next sentence)\n",
        "#         if self.is_testset:\n",
        "#             shift_txt = pd.concat([df['TEST_SECTION_ID'][1:], pd.Series([0])], ignore_index=True) - df['TEST_SECTION_ID']\n",
        "#             mask = list((shift_txt == 0)[:-1]) + [True]\n",
        "#         else:\n",
        "#             mask = df['I2'] != 0\n",
        "#         df = df.loc[mask]\n",
        "#         ## cleaning irrelavant info\n",
        "#         df = df.drop(columns=['PRESS_TIME', 'RELEASE_TIME', 'KEYCODE', 'INDEX', 'TEST_SECTION_ID'])\n",
        "#         df = df.iloc[:-1, :]\n",
        "#         return df\n",
        "    \n",
        "#     ## https://towardsdatascience.com/do-you-use-apply-in-pandas-there-is-a-600x-faster-way-d2497facfa66\n",
        "#     def digraph_avg(self, avg_mode, data=None, drop_origin=True, rename_avg=True, round_avg=True):\n",
        "#         if data:\n",
        "#             df = data.copy()\n",
        "#         else:\n",
        "#             df = self.digraph.copy()\n",
        "#         df['K1_K2'] = df[['K1', 'K2']].apply(tuple, axis=1)\n",
        "#         latencies = self.latencies.copy()\n",
        "#         if 'HL' in latencies:\n",
        "#             latencies.remove('HL')\n",
        "#             latencies.insert(0, 'HL2')\n",
        "#             latencies.insert(0, 'HL1')\n",
        "#         for XL in latencies:\n",
        "#             df[XL+'_avg'] = df[XL]\n",
        "#         for pair in df['K1_K2'].unique():\n",
        "#             mask = df['K1_K2'] == pair\n",
        "#             if avg_mode == 'mean':\n",
        "#                 avg_df = df.loc[mask, latencies].mean()\n",
        "#             else:\n",
        "#                 avg_df = df.loc[mask, latencies].median()\n",
        "#             for XL in latencies:\n",
        "#                 df.loc[mask, XL+'_avg'] = avg_df[XL]\n",
        "#         if round_avg:\n",
        "#             for XL in latencies:\n",
        "#                 df[XL+'_avg'] = round(df[XL+'_avg'])\n",
        "#         if drop_origin:\n",
        "#             df = df.drop(columns=latencies+['K1_K2'])\n",
        "#         if drop_origin and rename_avg:\n",
        "#             df = df.rename(columns=lambda name: re.search(r'(.{2,3})(_avg)', name).group(1) if '_avg' in name else name)\n",
        "#         return df\n",
        "\n",
        "#     def unigraph_avg(self, avg_mode, data=None, drop_origin=True, rename_avg=True, round_avg=True):\n",
        "#         if data:\n",
        "#             df = data.copy()\n",
        "#         else:\n",
        "#             df = self.unigraph.copy()\n",
        "#         for XL in self.latencies:\n",
        "#             df[XL+'_avg'] = df[XL]\n",
        "#         for keycode in df['KEYCODE'].unique():\n",
        "#             mask = df['KEYCODE'] == keycode\n",
        "#             if avg_mode == 'mean':\n",
        "#                 avg_df = df.loc[mask, self.latencies].mean()\n",
        "#             else:\n",
        "#                 avg_df = df.loc[mask, self.latencies].median()\n",
        "#             for XL in self.latencies:\n",
        "#                 df.loc[mask, XL+'_avg'] = avg_df[XL]\n",
        "#         if round_avg:\n",
        "#             for XL in self.latencies:\n",
        "#                 df[XL+'_avg'] = round(df[XL+'_avg'])\n",
        "#         if drop_origin:\n",
        "#             df = df.drop(columns=self.latencies)\n",
        "#         if drop_origin and rename_avg:\n",
        "#             df = df.rename(columns=lambda name: name[:2] if '_avg' in name else name)\n",
        "#         return df\n",
        "    \n",
        "#     def digraph_encode_keyboard(self):\n",
        "#         df = self.digraph\n",
        "#         keycode_dist = []\n",
        "#         home_dist = []\n",
        "#         for row in df.index:\n",
        "#             keycode_dist.append(self.keyboard_dict['keycode'](df['K1'][row], df['K2'][row]))\n",
        "#             home_dist.append(self.keyboard_dict['home']([df['K1'][row], df['K2'][row]]))\n",
        "#         df['KD'] = keycode_dist\n",
        "#         df['HD'] = home_dist\n",
        "#         cols = list(df.columns[:-5]) + list(df.columns[-2:]) + list(df.columns[-5:-2])\n",
        "#         df = df[cols]\n",
        "#         return df\n",
        "    \n",
        "#     def unigraph_encode_keyboard(self):\n",
        "#         df = self.unigraph\n",
        "#         home_dist = []\n",
        "#         for row in df.index:\n",
        "#             home_dist.append(self.keyboard_dict['home']([df['KEYCODE'][row]]))\n",
        "#         df['HD'] = home_dist\n",
        "#         cols = list(df.columns[:-3]) + list(df.columns[-1:]) + list(df.columns[-3:-1])\n",
        "#         df = df[cols]\n",
        "#         return df\n",
        "    \n",
        "#     def filter_by_IQRs(self, data, folds, latencies):\n",
        "#         df = data.copy()\n",
        "#         for fold, latency in zip(folds, latencies):\n",
        "#             for user in data['USER'].unique():\n",
        "#                 mask_user = data['USER'] == user\n",
        "#                 mask_non_user = data['USER'] != user\n",
        "#                 Q3 = data.loc[mask_user, latency].quantile(.75)\n",
        "#                 Q1 = data.loc[mask_user, latency].quantile(.25)\n",
        "#                 IQR = Q3 - Q1\n",
        "#                 max = Q3 + fold * IQR\n",
        "#                 min = Q1 - fold * IQR\n",
        "#                 mask_max = data[latency] <= max\n",
        "#                 mask_min = data[latency] >= min\n",
        "#                 df = df.loc[mask_user & mask_max & mask_min | mask_non_user]\n",
        "#         return df"
      ],
      "metadata": {
        "id": "iW-P5uU4fy8g"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Deprecated : testset extractor has problems (sentence transition timestamps extraction)\n",
        "\n",
        "# class Extractors:\n",
        "#     def __init__(self, sub_data, keyboard_dict, latencies):\n",
        "#         self.keyboard_dict = keyboard_dict\n",
        "#         self.latencies = latencies\n",
        "#         # self.is_testset = is_testset\n",
        "\n",
        "#         self.unigraph = self.unigraph_extractor(sub_data)\n",
        "#         self.digraph = self.digraph_extractor(sub_data)\n",
        "    \n",
        "#     def unigraph_extractor(self, df, user_int=True, keycode_str=True, drop_user=False):\n",
        "#         df = df[['PARTICIPANT_ID', 'TEST_SECTION_ID', 'PRESS_TIME', 'RELEASE_TIME', 'KEYCODE', 'INDEX']]\n",
        "#         df = df.astype('float64')\n",
        "#         if user_int:\n",
        "#             df['PARTICIPANT_ID'] = df['PARTICIPANT_ID'].astype('int64')\n",
        "#         df = df.rename(columns={'PARTICIPANT_ID': 'USER'})\n",
        "#         if drop_user:\n",
        "#             df = df.drop(columns=['USER'])\n",
        "#         if keycode_str:\n",
        "#             df['KEYCODE'] = df['KEYCODE'].astype('int64').astype(str)\n",
        "#         ## construct new features\n",
        "#         if 'HL' in self.latencies:\n",
        "#             df['HL'] = df['RELEASE_TIME'] - df['PRESS_TIME']\n",
        "#         if 'IL' in self.latencies:\n",
        "#             df['IL'] = pd.concat([df['PRESS_TIME'][1:], pd.Series([0])], ignore_index=True) - df['RELEASE_TIME']\n",
        "#         if 'RL' in self.latencies:\n",
        "#             df['RL'] = pd.concat([df['RELEASE_TIME'][1:], pd.Series([0])], ignore_index=True) - df['RELEASE_TIME']\n",
        "#         if 'PL' in self.latencies:\n",
        "#             df['PL'] = pd.concat([df['PRESS_TIME'][1:], pd.Series([0])], ignore_index=True) - df['PRESS_TIME']\n",
        "#         # ## dropping rows where the NEXT row has INDEX==0 (indicating a transition to next sentence)\n",
        "#         # if self.is_testset:\n",
        "#         #     shift_txt = pd.concat([df['TEST_SECTION_ID'][1:], pd.Series([0])], ignore_index=True) - df['TEST_SECTION_ID']\n",
        "#         #     mask = list((shift_txt == 0)[:-1]) + [True]\n",
        "#         # else:\n",
        "#         #     mask =  list((df['INDEX'] != 0)[1:]) + [False]\n",
        "#         # df = df.loc[mask]\n",
        "#         ## cleaning irrelavant info\n",
        "#         df = df.drop(columns=['PRESS_TIME', 'RELEASE_TIME', 'TEST_SECTION_ID'])\n",
        "#         df = df.iloc[:-1, :]\n",
        "#         return df\n",
        "    \n",
        "#     def digraph_extractor(self, df, user_int=True, keycode_str=True, drop_user=False):\n",
        "#         df = df[['PARTICIPANT_ID', 'TEST_SECTION_ID', 'PRESS_TIME', 'RELEASE_TIME', 'KEYCODE', 'INDEX']]\n",
        "#         df = df.astype('float64')\n",
        "#         if user_int:\n",
        "#             df['PARTICIPANT_ID'] = df['PARTICIPANT_ID'].astype('int64')\n",
        "#         df = df.rename(columns={'PARTICIPANT_ID': 'USER'})\n",
        "#         if drop_user:\n",
        "#             df = df.drop(columns=['USER'])\n",
        "#         ## construct new features\n",
        "#         df['K1'] = df['KEYCODE']\n",
        "#         df['K2'] = pd.concat([df['KEYCODE'][1:], pd.Series([0])], ignore_index=True)\n",
        "#         if keycode_str:\n",
        "#             df['K1'] = df['K1'].astype('int64').astype(str)\n",
        "#             df['K2'] = df['K2'].astype('int64').astype(str)\n",
        "#         df['I1'] = df['INDEX']\n",
        "#         df['I2'] = pd.concat([df['INDEX'][1:], pd.Series([0])], ignore_index=True)\n",
        "#         if 'HL' in self.latencies:\n",
        "#             df['HL1'] = df['RELEASE_TIME'] - df['PRESS_TIME']\n",
        "#             df['HL2'] = pd.concat([df['HL1'][1:], pd.Series([0])], ignore_index=True)\n",
        "#         if 'IL' in self.latencies:\n",
        "#             df['IL'] = pd.concat([df['PRESS_TIME'][1:], pd.Series([0])], ignore_index=True) - df['RELEASE_TIME']\n",
        "#         if 'RL' in self.latencies:\n",
        "#             df['RL'] = pd.concat([df['RELEASE_TIME'][1:], pd.Series([0])], ignore_index=True) - df['RELEASE_TIME']\n",
        "#         if 'PL' in self.latencies:\n",
        "#             df['PL'] = pd.concat([df['PRESS_TIME'][1:], pd.Series([0])], ignore_index=True) - df['PRESS_TIME']\n",
        "#         # ## dropping instances where I2 is zero (indicating a transition to next sentence)\n",
        "#         # if self.is_testset:\n",
        "#         #     shift_txt = pd.concat([df['TEST_SECTION_ID'][1:], pd.Series([0])], ignore_index=True) - df['TEST_SECTION_ID']\n",
        "#         #     mask = list((shift_txt == 0)[:-1]) + [True]\n",
        "#         # else:\n",
        "#         #     mask = df['I2'] != 0\n",
        "#         # df = df.loc[mask]\n",
        "#         ## cleaning irrelavant info\n",
        "#         df = df.drop(columns=['PRESS_TIME', 'RELEASE_TIME', 'KEYCODE', 'INDEX', 'TEST_SECTION_ID'])\n",
        "#         df = df.iloc[:-1, :]\n",
        "#         return df\n",
        "    \n",
        "#     ## https://towardsdatascience.com/do-you-use-apply-in-pandas-there-is-a-600x-faster-way-d2497facfa66\n",
        "#     def digraph_avg(self, avg_mode, data=None, drop_origin=True, rename_avg=True, round_avg=True):\n",
        "#         if data:\n",
        "#             df = data.copy()\n",
        "#         else:\n",
        "#             df = self.digraph.copy()\n",
        "#         df['K1_K2'] = df[['K1', 'K2']].apply(tuple, axis=1)\n",
        "#         latencies = self.latencies.copy()\n",
        "#         if 'HL' in latencies:\n",
        "#             latencies.remove('HL')\n",
        "#             latencies.insert(0, 'HL2')\n",
        "#             latencies.insert(0, 'HL1')\n",
        "#         for XL in latencies:\n",
        "#             df[XL+'_avg'] = df[XL]\n",
        "#         for pair in df['K1_K2'].unique():\n",
        "#             mask = df['K1_K2'] == pair\n",
        "#             if avg_mode == 'mean':\n",
        "#                 avg_df = df.loc[mask, latencies].mean()\n",
        "#             else:\n",
        "#                 avg_df = df.loc[mask, latencies].median()\n",
        "#             for XL in latencies:\n",
        "#                 df.loc[mask, XL+'_avg'] = avg_df[XL]\n",
        "#         if round_avg:\n",
        "#             for XL in latencies:\n",
        "#                 df[XL+'_avg'] = round(df[XL+'_avg'])\n",
        "#         if drop_origin:\n",
        "#             df = df.drop(columns=latencies+['K1_K2'])\n",
        "#         if drop_origin and rename_avg:\n",
        "#             df = df.rename(columns=lambda name: re.search(r'(.{2,3})(_avg)', name).group(1) if '_avg' in name else name)\n",
        "#         return df\n",
        "\n",
        "#     def unigraph_avg(self, avg_mode, data=None, drop_origin=True, rename_avg=True, round_avg=True):\n",
        "#         if data:\n",
        "#             df = data.copy()\n",
        "#         else:\n",
        "#             df = self.unigraph.copy()\n",
        "#         for XL in self.latencies:\n",
        "#             df[XL+'_avg'] = df[XL]\n",
        "#         for keycode in df['KEYCODE'].unique():\n",
        "#             mask = df['KEYCODE'] == keycode\n",
        "#             if avg_mode == 'mean':\n",
        "#                 avg_df = df.loc[mask, self.latencies].mean()\n",
        "#             else:\n",
        "#                 avg_df = df.loc[mask, self.latencies].median()\n",
        "#             for XL in self.latencies:\n",
        "#                 df.loc[mask, XL+'_avg'] = avg_df[XL]\n",
        "#         if round_avg:\n",
        "#             for XL in self.latencies:\n",
        "#                 df[XL+'_avg'] = round(df[XL+'_avg'])\n",
        "#         if drop_origin:\n",
        "#             df = df.drop(columns=self.latencies)\n",
        "#         if drop_origin and rename_avg:\n",
        "#             df = df.rename(columns=lambda name: name[:2] if '_avg' in name else name)\n",
        "#         return df\n",
        "    \n",
        "#     def digraph_encode_keyboard(self):\n",
        "#         df = self.digraph\n",
        "#         keycode_dist = []\n",
        "#         home_dist = []\n",
        "#         for row in df.index:\n",
        "#             keycode_dist.append(self.keyboard_dict['keycode'](df['K1'][row], df['K2'][row]))\n",
        "#             home_dist.append(self.keyboard_dict['home']([df['K1'][row], df['K2'][row]]))\n",
        "#         df['KD'] = keycode_dist\n",
        "#         df['HD'] = home_dist\n",
        "#         cols = list(df.columns[:-5]) + list(df.columns[-2:]) + list(df.columns[-5:-2])\n",
        "#         df = df[cols]\n",
        "#         return df\n",
        "    \n",
        "#     def unigraph_encode_keyboard(self):\n",
        "#         df = self.unigraph\n",
        "#         home_dist = []\n",
        "#         for row in df.index:\n",
        "#             home_dist.append(self.keyboard_dict['home']([df['KEYCODE'][row]]))\n",
        "#         df['HD'] = home_dist\n",
        "#         cols = list(df.columns[:-3]) + list(df.columns[-1:]) + list(df.columns[-3:-1])\n",
        "#         df = df[cols]\n",
        "#         return df\n",
        "    \n",
        "#     def filter_by_IQRs(self, data, folds, latencies):\n",
        "#         df = data.copy()\n",
        "#         for fold, latency in zip(folds, latencies):\n",
        "#             for user in data['USER'].unique():\n",
        "#                 mask_user = data['USER'] == user\n",
        "#                 mask_non_user = data['USER'] != user\n",
        "#                 Q3 = data.loc[mask_user, latency].quantile(.75)\n",
        "#                 Q1 = data.loc[mask_user, latency].quantile(.25)\n",
        "#                 IQR = Q3 - Q1\n",
        "#                 max = Q3 + fold * IQR\n",
        "#                 min = Q1 - fold * IQR\n",
        "#                 mask_max = data[latency] <= max\n",
        "#                 mask_min = data[latency] >= min\n",
        "#                 df = df.loc[mask_user & mask_max & mask_min | mask_non_user]\n",
        "#         return df\n",
        "    \n",
        "#     # def unigraph_filter_outliers(self, quantile_perc):\n",
        "#     #     data = self.unigraph.copy()\n",
        "#     #     return data.loc[(data['HL'] <= data['HL'].quantile(quantile_perc))\n",
        "#     #                   & (data['PL'] <= data['PL'].quantile(quantile_perc))]\n",
        "    \n",
        "#     # def digraph_filter_outliers(self, quantile_perc):\n",
        "#     #     data = self.digraph.copy()\n",
        "#     #     return data.loc[(data['HL1'] <= data['HL1'].quantile(quantile_perc))\n",
        "#     #                   & (data['HL2'] <= data['HL2'].quantile(quantile_perc))\n",
        "#     #                   & (data['PL'] <= data['PL'].quantile(quantile_perc))]"
      ],
      "metadata": {
        "id": "nSMwleY1gPCz"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## DEPRECATED: delete key_only (which is just unigraph with 1 output time latency)\n",
        "\n",
        "# class Extractors:\n",
        "#     def __init__(self, sub_data, keyboard_dict, avg_mode, add_layout, remove_outliers, conn_latency):\n",
        "#         self.keyboard_dict = keyboard_dict\n",
        "#         self.add_layout = add_layout\n",
        "#         self.XL = conn_latency\n",
        "#         self.outliers_perc = remove_outliers\n",
        "#         self.avg_mode = avg_mode        ## takes value in ['mean', 'median', None/False]\n",
        "\n",
        "#         self.unigraph = self.unigraph_extractor(sub_data)\n",
        "#         self.digraph = self.digraph_extractor(sub_data)\n",
        "\n",
        "#         if self.outliers_perc:\n",
        "#             self.unigraph = self.unigraph_filter_outliers(self.outliers_perc)\n",
        "#             self.digraph = self.digraph_filter_outliers(self.outliers_perc)\n",
        "\n",
        "#         if avg_mode:\n",
        "#             self.unigraph = self.unigraph_avg()\n",
        "#             self.digraph = self.digraph_avg()\n",
        "        \n",
        "#         if add_layout:\n",
        "#             self.unigraph = self.unigraph_encode_keyboard()\n",
        "#             self.digraph = self.digraph_encode_keyboard()\n",
        "        \n",
        "#         self.key_only = self.key_extractor()\n",
        "    \n",
        "#     def unigraph_extractor(self, df, user_int=True, keycode_int=True, drop_user=False):\n",
        "#         df = df[['PARTICIPANT_ID', 'PRESS_TIME', 'RELEASE_TIME', 'KEYCODE', 'INDEX']]\n",
        "#         df = df.astype('float64')\n",
        "#         if user_int:\n",
        "#             df['PARTICIPANT_ID'] = df['PARTICIPANT_ID'].astype('int64')\n",
        "#         df = df.rename(columns={'PARTICIPANT_ID': 'USER'})\n",
        "#         if drop_user:\n",
        "#             df = df.drop(columns=['USER'])\n",
        "#         if keycode_int:\n",
        "#             df['KEYCODE'] = df['KEYCODE'].astype('int64')\n",
        "#         ## construct new features\n",
        "#         df['HL'] = df['RELEASE_TIME'] - df['PRESS_TIME']\n",
        "#         if self.XL == 'IL':\n",
        "#             df['IL'] = pd.concat([df['PRESS_TIME'][1:], pd.Series([0])], ignore_index=True) - df['RELEASE_TIME']\n",
        "#         elif self.XL == 'RL':\n",
        "#             df['RL'] = pd.concat([df['RELEASE_TIME'][1:], pd.Series([0])], ignore_index=True) - df['RELEASE_TIME']\n",
        "#         else:\n",
        "#             df['PL'] = pd.concat([df['PRESS_TIME'][1:], pd.Series([0])], ignore_index=True) - df['PRESS_TIME']\n",
        "#         ## cleaning irrelavant info\n",
        "#         df = df.drop(columns=['PRESS_TIME', 'RELEASE_TIME'])\n",
        "#         df = df.iloc[:-1, :]\n",
        "#         return df\n",
        "    \n",
        "#     def digraph_extractor(self, df, user_int=True, keycode_int=True, drop_user=False):\n",
        "#         df = df[['PARTICIPANT_ID', 'PRESS_TIME', 'RELEASE_TIME', 'KEYCODE', 'INDEX']]\n",
        "#         df = df.astype('float64')\n",
        "#         if user_int:\n",
        "#             df['PARTICIPANT_ID'] = df['PARTICIPANT_ID'].astype('int64')\n",
        "#         df = df.rename(columns={'PARTICIPANT_ID': 'USER'})\n",
        "#         if drop_user:\n",
        "#             df = df.drop(columns=['USER'])\n",
        "#         ## construct new features\n",
        "#         df['K1'] = df['KEYCODE']\n",
        "#         df['K2'] = pd.concat([df['KEYCODE'][1:], pd.Series([0])], ignore_index=True)\n",
        "#         if keycode_int:\n",
        "#             df['K1'] = df['K1'].astype('int64')\n",
        "#             df['K2'] = df['K2'].astype('int64')\n",
        "#         df['I1'] = df['INDEX']\n",
        "#         df['I2'] = pd.concat([df['INDEX'][1:], pd.Series([0])], ignore_index=True)\n",
        "#         df['HL1'] = df['RELEASE_TIME'] - df['PRESS_TIME']\n",
        "#         df['HL2'] = pd.concat([df['HL1'][1:], pd.Series([0])], ignore_index=True)\n",
        "#         if self.XL == 'IL':\n",
        "#             df['IL'] = pd.concat([df['PRESS_TIME'][1:], pd.Series([0])], ignore_index=True) - df['RELEASE_TIME']\n",
        "#         elif self.XL == 'RL':\n",
        "#             df['RL'] = pd.concat([df['RELEASE_TIME'][1:], pd.Series([0])], ignore_index=True) - df['RELEASE_TIME']\n",
        "#         else:\n",
        "#             df['PL'] = pd.concat([df['PRESS_TIME'][1:], pd.Series([0])], ignore_index=True) - df['PRESS_TIME']\n",
        "#         ## cleaning irrelavant info\n",
        "#         df = df.drop(columns=['PRESS_TIME', 'RELEASE_TIME', 'KEYCODE', 'INDEX'])\n",
        "#         df = df.iloc[:-1, :]\n",
        "#         return df\n",
        "    \n",
        "#     def key_extractor(self):\n",
        "#         return self.unigraph.drop(columns=[self.XL])\n",
        "    \n",
        "#     ## https://towardsdatascience.com/do-you-use-apply-in-pandas-there-is-a-600x-faster-way-d2497facfa66\n",
        "#     def digraph_avg(self, drop_origin=True, rename_avg=True, round_avg=True):\n",
        "#         df = self.digraph\n",
        "#         df['K1_K2'] = df[['K1', 'K2']].apply(tuple, axis=1)\n",
        "#         df['HL1_avg'] = df['HL1']\n",
        "#         df['HL2_avg'] = df['HL2']\n",
        "#         df[self.XL+'_avg'] = df[self.XL]\n",
        "#         for pair in df['K1_K2'].unique():\n",
        "#             if self.avg_mode == 'mean':\n",
        "#                 avg_df = df[df['K1_K2'] == pair][['HL1', self.XL, 'HL2']].mean()\n",
        "#             else:\n",
        "#                 avg_df = df[df['K1_K2'] == pair][['HL1', self.XL, 'HL2']].median()\n",
        "#             mask = df['K1_K2'] == pair\n",
        "#             df.loc[mask, 'HL1_avg'] = avg_df['HL1']\n",
        "#             df.loc[mask, 'HL2_avg'] = avg_df['HL2']\n",
        "#             df.loc[mask, self.XL+'_avg'] = avg_df[self.XL]\n",
        "#         if round_avg:\n",
        "#             df['HL1_avg'] = round(df['HL1_avg'])\n",
        "#             df['HL2_avg'] = round(df['HL2_avg'])\n",
        "#             df[self.XL+'_avg'] = round(df[self.XL+'_avg'])\n",
        "#         if drop_origin:\n",
        "#             df = df.drop(columns=['HL1', 'HL2', self.XL, 'K1_K2'])\n",
        "#         if drop_origin and rename_avg:\n",
        "#             df = df.rename(columns={'HL1_avg':'HL1', 'HL2_avg':'HL2', self.XL+'_avg':self.XL})\n",
        "#         return df\n",
        "\n",
        "#     def unigraph_avg(self, drop_origin=True, rename_avg=True, round_avg=True):\n",
        "#         df = self.unigraph\n",
        "#         df['HL_avg'] = df['HL']\n",
        "#         df[self.XL+'_avg'] = df[self.XL]\n",
        "#         for keycode in df['KEYCODE'].unique():\n",
        "#             if self.avg_mode == 'mean':\n",
        "#                 avg_df = df[df['KEYCODE'] == keycode][['HL', self.XL]].mean()\n",
        "#             else:\n",
        "#                 avg_df = df[df['KEYCODE'] == keycode][['HL', self.XL]].median()\n",
        "#             mask = df['KEYCODE'] == keycode\n",
        "#             df.loc[mask, 'HL_avg'] = avg_df['HL']\n",
        "#             df.loc[mask, self.XL+'_avg'] = avg_df[self.XL]\n",
        "#         if round_avg:\n",
        "#             df['HL_avg'] = round(df['HL_avg'])\n",
        "#             df[self.XL+'_avg'] = round(df[self.XL+'_avg'])\n",
        "#         if drop_origin:\n",
        "#             df = df.drop(columns=['HL', self.XL])\n",
        "#         if drop_origin and rename_avg:\n",
        "#             df = df.rename(columns={'HL_avg':'HL', self.XL+'_avg':self.XL})\n",
        "#         return df\n",
        "    \n",
        "#     def digraph_encode_keyboard(self):\n",
        "#         df = self.digraph\n",
        "#         keycode_dist = []\n",
        "#         home_dist = []\n",
        "#         for row in df.index:\n",
        "#             keycode_dist.append(self.keyboard_dict['keycode'](df['K1'][row], df['K2'][row]))\n",
        "#             home_dist.append(self.keyboard_dict['home']([df['K1'][row], df['K2'][row]]))\n",
        "#         df['KD'] = keycode_dist\n",
        "#         df['HD'] = home_dist\n",
        "#         cols = list(df.columns[:-5]) + list(df.columns[-2:]) + list(df.columns[-5:-2])\n",
        "#         df = df[cols]\n",
        "#         return df\n",
        "    \n",
        "#     def unigraph_encode_keyboard(self):\n",
        "#         df = self.unigraph\n",
        "#         home_dist = []\n",
        "#         for row in df.index:\n",
        "#             home_dist.append(self.keyboard_dict['home']([df['KEYCODE'][row]]))\n",
        "#         df['HD'] = home_dist\n",
        "#         cols = list(df.columns[:-3]) + list(df.columns[-1:]) + list(df.columns[-3:-1])\n",
        "#         df = df[cols]\n",
        "#         return df\n",
        "    \n",
        "#     def unigraph_filter_outliers(self, quantile_perc):\n",
        "#         data = self.unigraph\n",
        "#         return data.loc[(data['HL'] <= data['HL'].quantile(quantile_perc))\n",
        "#                       & (data['PL'] <= data['PL'].quantile(quantile_perc))]\n",
        "    \n",
        "#     def digraph_filter_outliers(self, quantile_perc):\n",
        "#         data = self.digraph\n",
        "#         return data.loc[(data['HL1'] <= data['HL1'].quantile(quantile_perc))\n",
        "#                       & (data['HL2'] <= data['HL2'].quantile(quantile_perc))\n",
        "#                       & (data['PL'] <= data['PL'].quantile(quantile_perc))]"
      ],
      "metadata": {
        "id": "pl5w7P9HyZ_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KDS - Sequential Input Generator"
      ],
      "metadata": {
        "id": "vkw_gAFZgQnQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KDS:\n",
        "    def __init__(self, df, \n",
        "                 n_steps, shift, batch_size, \n",
        "                 nonkeycodeB_features, output_features,\n",
        "                 encoders, enc_names, do_onehot=True):\n",
        "        self.df = df\n",
        "        self.window_length = n_steps + 1\n",
        "        self.n_steps = n_steps\n",
        "        self.shift = shift\n",
        "        self.batch = batch_size\n",
        "\n",
        "        self.inputB_features = nonkeycodeB_features        ## inputB_features are the features input to keycode_embed layer, not including keycodes\n",
        "        self.output_features = output_features\n",
        "        \n",
        "        for i, user in enumerate(self.df['USER'].unique()):\n",
        "            mask = self.df['USER'] == user\n",
        "            user_df = self.df.loc[mask, :]\n",
        "            ## One-hot on 'KEYCODE' (mode=='uni') OR 'K1', 'K2' (mode!='uni')\n",
        "            if len(encoders) == 1 and enc_names[0] == 'KEYCODE':\n",
        "                keycode_np = encoders[0].transform(user_df[['KEYCODE']].astype(str)).toarray()\n",
        "            elif len(encoders) == 1 and enc_names[0] == 'K1_K2':\n",
        "                keycode_np = encoders[0].transform(user_df[['K1', 'K2']].astype(str)).toarray()\n",
        "            else:\n",
        "                k1_onehot = encoders[0].transform(user_df[['K1']].astype(str)).toarray()\n",
        "                k2_onehot = encoders[1].transform(user_df[['K2']].astype(str)).toarray()\n",
        "                keycode_np = k1_onehot + k2_onehot\n",
        "            curr_df = np.concatenate([keycode_np, self.df.loc[mask, self.inputB_features+self.output_features]], axis=1)\n",
        "            ## get the TFDS dataset of inputs (inputA, inputB) and output\n",
        "            curr_in, curr_out = self.get_dataset(curr_df)\n",
        "            if i == 0:\n",
        "                self.ds_in = curr_in\n",
        "                self.ds_out = curr_out\n",
        "            else:\n",
        "                self.ds_in = self.ds_in.concatenate(curr_in)\n",
        "                self.ds_out = self.ds_out.concatenate(curr_out)\n",
        "        ## zip the TFDS inputs and output for easy access at training\n",
        "        self.ds = tf.data.Dataset.zip((self.ds_in, self.ds_out))\n",
        "        \n",
        "        for inputA, inputB in self.ds_in.take(1):\n",
        "            self.inputA = inputA.shape\n",
        "            self.inputB = inputB.shape\n",
        "        \n",
        "        for output in self.ds_out.take(1):\n",
        "            self.output = output.shape\n",
        "\n",
        "    def get_dataset(self, df):\n",
        "        dataset = tf.data.Dataset.from_tensor_slices(df).window(size=self.window_length, shift=self.shift, drop_remainder=True)\n",
        "        dataset = dataset.flat_map(lambda window: window.batch(self.window_length)).batch(self.batch)\n",
        "        ds_in = dataset.map(lambda window: (window[:, :self.n_steps, :], window[:, -1, :-len(self.output_features)]))\n",
        "        ds_in = ds_in.prefetch(tf.data.AUTOTUNE)\n",
        "        ds_out = dataset.map(lambda window: window[:, -1, -len(self.output_features):])\n",
        "        ds_out = ds_out.prefetch(tf.data.AUTOTUNE)\n",
        "        return ds_in, ds_out"
      ],
      "metadata": {
        "id": "GqQs2HbktOhw"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class KDS:\n",
        "#     def __init__(self, df, output_dim, n_steps, shift, batch_size, encoders, enc_names, do_onehot=True):\n",
        "#         self.df = df\n",
        "#         self.window_length = n_steps + 1\n",
        "#         self.n_steps = n_steps\n",
        "#         self.shift = shift\n",
        "#         self.batch = batch_size\n",
        "#         self.output_dim = output_dim\n",
        "        \n",
        "#         for i, user in enumerate(self.df['USER'].unique()):\n",
        "#             mask = self.df['USER'] == user\n",
        "#             curr_df = self.df.loc[mask, :].drop(columns=['USER'])\n",
        "#             ## One-hot on 'KEYCODE'\n",
        "#             if len(encoders) == 1 and enc_names[0] == 'KEYCODE':\n",
        "#                 curr_df = np.concatenate([encoders[0].transform(curr_df[['KEYCODE']].astype(str)).toarray(), curr_df.drop(columns=['KEYCODE'])], axis=1)\n",
        "#             elif len(encoders) == 1 and enc_names[0] == 'K1_K2':\n",
        "#                 curr_df = np.concatenate([encoders[0].transform(curr_df[['K1', 'K2']].astype(str)).toarray(), curr_df.drop(columns=['K1', 'K2'])], axis=1)\n",
        "#             else:\n",
        "#                 k1_onehot = encoders[0].transform(curr_df[['K1']].astype(str)).toarray()\n",
        "#                 k2_onehot = encoders[1].transform(curr_df[['K2']].astype(str)).toarray()\n",
        "#                 curr_df = np.concatenate([k1_onehot+k2_onehot, curr_df.drop(columns=['K1', 'K2'])], axis=1)\n",
        "#             ## get the TFDS dataset of inputs (inputA, inputB) and output\n",
        "#             curr_in, curr_out = self.get_dataset(curr_df)\n",
        "#             if i == 0:\n",
        "#                 self.ds_in = curr_in\n",
        "#                 self.ds_out = curr_out\n",
        "#             else:\n",
        "#                 self.ds_in = self.ds_in.concatenate(curr_in)\n",
        "#                 self.ds_out = self.ds_out.concatenate(curr_out)\n",
        "#         ## zip the TFDS inputs and output for easy access at training\n",
        "#         self.ds = tf.data.Dataset.zip((self.ds_in, self.ds_out))\n",
        "        \n",
        "#         for inputA, inputB in self.ds_in.take(1):\n",
        "#             self.inputA = inputA.shape\n",
        "#             self.inputB = inputB.shape\n",
        "        \n",
        "#         for output in self.ds_out.take(1):\n",
        "#             self.output = output.shape\n",
        "\n",
        "#     def get_dataset(self, df):\n",
        "#         dataset = tf.data.Dataset.from_tensor_slices(df).window(size=self.window_length, shift=self.shift, drop_remainder=True)\n",
        "#         dataset = dataset.flat_map(lambda window: window.batch(self.window_length)).batch(self.batch)\n",
        "#         ds_in = dataset.map(lambda window: (window[:, :self.n_steps, :], window[:, -1, :-self.output_dim]))\n",
        "#         ds_in = ds_in.prefetch(tf.data.AUTOTUNE)\n",
        "#         ds_out = dataset.map(lambda window: window[:, -1, -self.output_dim:])\n",
        "#         ds_out = ds_out.prefetch(tf.data.AUTOTUNE)\n",
        "#         return ds_in, ds_out"
      ],
      "metadata": {
        "id": "zuSsMDBFicrh"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## DEPRECATED: modified needed argument names and updated one-hot encoding methods\n",
        "\n",
        "# class KDS:\n",
        "#     def __init__(self, df, n_steps, shift, batch_size, encoder, mode='uni', do_onehot=True):\n",
        "#         self.df = df\n",
        "#         self.window_length = n_steps + 1\n",
        "#         self.n_steps = n_steps\n",
        "#         self.shift = shift\n",
        "#         self.batch = batch_size\n",
        "        \n",
        "#         for i, user in enumerate(self.df['USER'].unique()):\n",
        "#             mask = self.df['USER'] == user\n",
        "#             curr_df = self.df.loc[mask, :].drop(columns=['USER'])\n",
        "#             ## One-hot on 'KEYCODE' (mode=='uni') OR 'K1', 'K2' (mode!='uni')\n",
        "#             if mode == 'uni':\n",
        "#                 curr_df = np.concatenate([encoder.transform(curr_df[['KEYCODE']].astype(str)).toarray(), curr_df.drop(columns=['KEYCODE'])], axis=1)\n",
        "#             else:\n",
        "#                 curr_df = np.concatenate([encoder.transform(curr_df[['K1', 'K2']].astype(str)).toarray(), curr_df.drop(columns=['K1', 'K2'])], axis=1)\n",
        "#             curr_in, curr_out = self.get_dataset(curr_df)\n",
        "#             if i == 0:\n",
        "#                 self.ds_in = curr_in\n",
        "#                 self.ds_out = curr_out\n",
        "#             else:\n",
        "#                 self.ds_in = self.ds_in.concatenate(curr_in)\n",
        "#                 self.ds_out = self.ds_out.concatenate(curr_out)\n",
        "#         self.ds = tf.data.Dataset.zip((self.ds_in, self.ds_out))\n",
        "        \n",
        "#         for inputA, inputB in self.ds_in.take(1):\n",
        "#             self.inputA = inputA.shape\n",
        "#             self.inputB = inputB.shape\n",
        "        \n",
        "#         for output in self.ds_out.take(1):\n",
        "#             self.output = output.shape\n",
        "\n",
        "#     def get_dataset(self, df):\n",
        "#         dataset = tf.data.Dataset.from_tensor_slices(df).window(size=self.window_length, shift=self.shift, drop_remainder=True)\n",
        "#         dataset = dataset.flat_map(lambda window: window.batch(self.window_length)).batch(self.batch)\n",
        "#         ds_in = dataset.map(lambda window: (window[:, :self.n_steps, :], window[:, -1, :-2]))\n",
        "#         ds_in = ds_in.prefetch(tf.data.AUTOTUNE)\n",
        "#         ds_out = dataset.map(lambda window: window[:, -1, -2:])\n",
        "#         ds_out = ds_out.prefetch(tf.data.AUTOTUNE)\n",
        "#         return ds_in, ds_out"
      ],
      "metadata": {
        "id": "F_Xm2L6QgXN6"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KDI - Image-like Input Generator"
      ],
      "metadata": {
        "id": "MvEqRMYGis9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KDI:\n",
        "    def __init__(self, train_data, df, \n",
        "                 n_steps, shift, batch_size, mat_length, \n",
        "                 inputA_features, inputB_features, output_features, \n",
        "                 inputB_type='image', encoders=None, keep_smaller_window=False, add_UNK=True):\n",
        "        self.train_data = train_data\n",
        "        self.df = df\n",
        "        self.n_steps = n_steps\n",
        "        self.shift = shift\n",
        "        self.batch_size = batch_size\n",
        "        self.mat_length = mat_length\n",
        "\n",
        "        self.inputA_features = inputA_features\n",
        "        self.inputB_features = inputB_features\n",
        "        self.inputB_type = inputB_type                ## 'label, 'onehot', or 'image', else default to 'image'\n",
        "        self.output_features = output_features        ## output_features ('HL', 'XL' in ['PL', 'RL', 'IL'])\n",
        "\n",
        "        self.encoders = encoders\n",
        "        self.keep_smaller_window = keep_smaller_window\n",
        "        self.add_UNK = add_UNK\n",
        "\n",
        "        self.keycode_dict = self.keycode_topfreq_dict(top=self.mat_length-1)\n",
        "\n",
        "        self.inputA, self.inputB, self.output = self.kdi_training_data()\n",
        "        self.ds = self.generate_kdi()\n",
        "\n",
        "\n",
        "    def keycode_topfreq_dict(self, top):\n",
        "        '''\n",
        "        generate dictionary for the most popular `top` many keycodes using training data\n",
        "        '''\n",
        "        keycode_dict = {keycode: i for i, keycode in enumerate(self.train_data['KEYCODE'].astype('int32').value_counts()[:top].to_dict().keys())}\n",
        "        if self.add_UNK:\n",
        "            keycode_dict[0] = len(keycode_dict)\n",
        "        return keycode_dict\n",
        "  \n",
        "\n",
        "    def single_input_image(self, curr_chunk, features, mat_length, keycode_dict):\n",
        "        '''\n",
        "        Helper function to generate a single image (with # of color channels == # of features)\n",
        "        '''\n",
        "        mat_dict = {}\n",
        "        for feature in features:\n",
        "            mat_dict['mat_'+feature] = np.zeros((mat_length, mat_length))\n",
        "        mat_dict['count'] = np.zeros((mat_length, mat_length))\n",
        "\n",
        "        for row in curr_chunk.index:\n",
        "            i = int(curr_chunk.loc[row, 'K1'])\n",
        "            j = int(curr_chunk.loc[row, 'K2'])\n",
        "            if i in keycode_dict:\n",
        "                pos_i = keycode_dict[i]\n",
        "            else:\n",
        "                pos_i = keycode_dict[0]   ## pos_i = top (the last key-value pair)\n",
        "            if j in keycode_dict:\n",
        "                pos_j = keycode_dict[j]\n",
        "            else:\n",
        "                pos_j = keycode_dict[0]\n",
        "            for feature in features:\n",
        "                if feature != 'HL':\n",
        "                    mat_dict['mat_'+feature][pos_i, pos_j] += curr_chunk.loc[row, feature]\n",
        "                else:\n",
        "                    mat_dict['mat_'+feature][pos_i, pos_j] += (i + j) / 2\n",
        "            mat_dict['count'][pos_i, pos_j] += 1\n",
        "        mask_nonzero = mat_dict['count'] != 0\n",
        "        mat_ls = []\n",
        "        for feature in features:\n",
        "            mat_dict['mat_'+feature][mask_nonzero] = mat_dict['mat_'+feature][mask_nonzero] / mat_dict['count'][mask_nonzero]\n",
        "            mat_ls.append(mat_dict['mat_'+feature])\n",
        "        return np.stack(mat_ls, axis=-1)\n",
        "    \n",
        "\n",
        "    def single_kdi_input(self, curr_chunk):\n",
        "        '''\n",
        "        Generates the group of inputA, inputB, and Output of the current chunk\n",
        "        '''\n",
        "        last_index = curr_chunk.index[-1]\n",
        "        output_ls = []\n",
        "        for feature in self.output_features:\n",
        "            output_ls.append(curr_chunk.loc[last_index, feature])\n",
        "        output_np = np.array(output_ls)\n",
        "        ## inputA \n",
        "        inputA = self.single_input_image(curr_chunk.iloc[:-1], self.inputA_features, self.mat_length, self.keycode_dict)\n",
        "        ## inputB\n",
        "        if self.inputB_type == 'label':\n",
        "            inputB = np.array(curr_chunk.loc[last_index, self.inputB_features + ['K1', 'K2']])\n",
        "        elif self.inputB_type == 'onehot' and self.encoders:\n",
        "            if len(self.encoders) == 1:\n",
        "                inputB_keycode = self.encoders[0].transform(curr_chunk.loc[[last_index], ['K1', 'K2']].astype(str)).toarray()\n",
        "            else:\n",
        "                inputB_k1 = self.encoders[0].transform(curr_chunk.loc[[last_index], ['K1']].astype(str)).toarray()\n",
        "                inputB_k2 = self.encoders[1].transform(curr_chunk.loc[[last_index], ['K2']].astype(str)).toarray()\n",
        "                inputB_keycode = inputB_k1 + inputB_k2\n",
        "            inputB = np.concatenate([inputB_keycode, np.array(curr_chunk.loc[last_index, self.inputB_features])], axis=1)\n",
        "        else:\n",
        "            inputB = self.single_input_image(curr_chunk.iloc[-1:], self.inputB_features, self.mat_length, self.keycode_dict)\n",
        "        return inputA, inputB, output_np\n",
        "    \n",
        "\n",
        "    def kdi_training_data(self):\n",
        "        '''\n",
        "        Generates numpy arrays of inputA=(total_images, mat_length, mat_length, # of features), inputB, output\n",
        "        '''\n",
        "        window_length = self.n_steps + 1\n",
        "        inputA_arr, inputB_arr, output_arr = [], [], []\n",
        "        for user in self.df['USER'].unique():\n",
        "            curr_df = self.df[self.df['USER'] == user]\n",
        "            i = 0\n",
        "            while i+window_length < len(curr_df):\n",
        "                curr_chunk = curr_df.iloc[i:i+window_length]\n",
        "                curr_inputA, curr_inputB, curr_output = self.single_kdi_input(curr_chunk)\n",
        "                inputA_arr.append(curr_inputA)\n",
        "                inputB_arr.append(curr_inputB)\n",
        "                output_arr.append(curr_output)\n",
        "                i += self.shift\n",
        "            if self.keep_smaller_window and i < len(curr_df) - 1:    ## i cannot be curr_df[-1:] of length 1, since impossible to split into input and output data\n",
        "                curr_chunk = curr_df.iloc[i:]\n",
        "                curr_inputA, curr_inputB, curr_output = self.single_kdi_input(curr_chunk)\n",
        "                inputA_arr.append(curr_inputA)\n",
        "                inputB_arr.append(curr_inputB)\n",
        "                output_arr.append(curr_output)\n",
        "        return np.stack(inputA_arr, axis=0), np.stack(inputB_arr, axis=0), np.stack(output_arr, axis=0)\n",
        "    \n",
        "\n",
        "    def generate_kdi(self):\n",
        "        '''\n",
        "        Prepared tf.data object for training (batched)\n",
        "        '''\n",
        "        dataset = tf.data.Dataset.from_tensor_slices(({'inputA': self.inputA, 'inputB': self.inputB}, \n",
        "                                                      self.output)).batch(self.batch_size)\n",
        "        return dataset"
      ],
      "metadata": {
        "id": "iZxi726Z0L1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## DEPRECATED: changed default inputB_type to 'image' in the code (i.e. the `else` column in if-elif-else ststement)\n",
        "## added docstrings\n",
        "\n",
        "# class KDI:\n",
        "#     def __init__(self, train_data, df, \n",
        "#                  n_steps, shift, batch_size, mat_length, \n",
        "#                  inputA_features, inputB_features, output_features, \n",
        "#                  inputB_type='image', encoders=None, keep_smaller_window=False, add_UNK=True):\n",
        "#         self.train_data = train_data\n",
        "#         self.df = df\n",
        "#         self.n_steps = n_steps\n",
        "#         self.shift = shift\n",
        "#         self.batch_size = batch_size\n",
        "#         self.mat_length = mat_length\n",
        "\n",
        "#         self.inputA_features = inputA_features\n",
        "#         self.inputB_features = inputB_features\n",
        "#         self.inputB_type = inputB_type                ## 'image', or 'onehot', else default to 'int'\n",
        "#         self.output_features = output_features        ## output_features ('HL', 'XL' in ['PL', 'RL', 'IL'])\n",
        "\n",
        "#         self.encoders = encoders\n",
        "#         self.keep_smaller_window = keep_smaller_window\n",
        "#         self.add_UNK = add_UNK\n",
        "\n",
        "#         self.keycode_dict = self.keycode_topfreq_dict(top=self.mat_length-1)\n",
        "\n",
        "#         self.inputA, self.inputB, self.output = self.kdi_training_data()\n",
        "#         self.ds = self.generate_kdi()\n",
        "\n",
        "\n",
        "#     def keycode_topfreq_dict(self, top):\n",
        "#         '''\n",
        "#         generate dictionary for the most popular `top` many keycodes using training data\n",
        "#         '''\n",
        "#         keycode_dict = {keycode: i for i, keycode in enumerate(self.train_data['KEYCODE'].astype('int32').value_counts()[:top].to_dict().keys())}\n",
        "#         if self.add_UNK:\n",
        "#             keycode_dict[0] = len(keycode_dict)\n",
        "#         return keycode_dict\n",
        "  \n",
        "\n",
        "#     def single_input_image(self, curr_chunk, features, mat_length, keycode_dict):\n",
        "#         mat_dict = {}\n",
        "#         for feature in features:\n",
        "#             mat_dict['mat_'+feature] = np.zeros((mat_length, mat_length))\n",
        "#         mat_dict['count'] = np.zeros((mat_length, mat_length))\n",
        "\n",
        "#         for row in curr_chunk.index:\n",
        "#             i = int(curr_chunk.loc[row, 'K1'])\n",
        "#             j = int(curr_chunk.loc[row, 'K2'])\n",
        "#             if i in keycode_dict:\n",
        "#                 pos_i = keycode_dict[i]\n",
        "#             else:\n",
        "#                 pos_i = keycode_dict[0]   ## pos_i = top (the last key-value pair)\n",
        "#             if j in keycode_dict:\n",
        "#                 pos_j = keycode_dict[j]\n",
        "#             else:\n",
        "#                 pos_j = keycode_dict[0]\n",
        "#             for feature in features:\n",
        "#                 if feature != 'HL':\n",
        "#                     mat_dict['mat_'+feature][pos_i, pos_j] += curr_chunk.loc[row, feature]\n",
        "#                 else:\n",
        "#                     mat_dict['mat_'+feature][pos_i, pos_j] += (i + j) / 2\n",
        "#             mat_dict['count'][pos_i, pos_j] += 1\n",
        "#         mask_nonzero = mat_dict['count'] != 0\n",
        "#         mat_ls = []\n",
        "#         for feature in features:\n",
        "#             mat_dict['mat_'+feature][mask_nonzero] = mat_dict['mat_'+feature][mask_nonzero] / mat_dict['count'][mask_nonzero]\n",
        "#             mat_ls.append(mat_dict['mat_'+feature])\n",
        "#         return np.stack(mat_ls, axis=-1)\n",
        "    \n",
        "\n",
        "#     def single_kdi_input(self, curr_chunk):\n",
        "#         last_index = curr_chunk.index[-1]\n",
        "#         output_ls = []\n",
        "#         for feature in self.output_features:\n",
        "#             output_ls.append(curr_chunk.loc[last_index, feature])\n",
        "#         output_np = np.array(output_ls)\n",
        "#         ## inputA \n",
        "#         inputA = self.single_input_image(curr_chunk.iloc[:-1], self.inputA_features, self.mat_length, self.keycode_dict)\n",
        "#         ## inputB\n",
        "#         if self.inputB_type == 'image':\n",
        "#             inputB = self.single_input_image(curr_chunk.iloc[-1:], self.inputB_features, self.mat_length, self.keycode_dict)\n",
        "#         elif self.inputB_type == 'onehot' and self.encoders:\n",
        "#             if len(self.encoders) == 1:\n",
        "#                 inputB_keycode = self.encoders[0].transform(curr_chunk.loc[[last_index], ['K1', 'K2']].astype(str)).toarray()\n",
        "#             else:\n",
        "#                 inputB_k1 = self.encoders[0].transform(curr_chunk.loc[[last_index], ['K1']].astype(str)).toarray()\n",
        "#                 inputB_k2 = self.encoders[1].transform(curr_chunk.loc[[last_index], ['K2']].astype(str)).toarray()\n",
        "#                 inputB_keycode = inputB_k1 + inputB_k2\n",
        "#             inputB = np.concatenate([inputB_keycode, np.array(curr_chunk.loc[last_index, self.inputB_features])], axis=1)\n",
        "#         else:\n",
        "#             inputB = np.array(curr_chunk.loc[last_index, self.inputB_features + ['K1', 'K2']])\n",
        "#         return inputA, inputB, output_np\n",
        "    \n",
        "\n",
        "#     def kdi_training_data(self):\n",
        "#         window_length = self.n_steps + 1\n",
        "#         inputA_arr, inputB_arr, output_arr = [], [], []\n",
        "#         for user in self.df['USER'].unique():\n",
        "#             curr_df = self.df[self.df['USER'] == user]\n",
        "#             i = 0\n",
        "#             while i+window_length < len(curr_df):\n",
        "#                 curr_chunk = curr_df.iloc[i:i+window_length]\n",
        "#                 curr_inputA, curr_inputB, curr_output = self.single_kdi_input(curr_chunk)\n",
        "#                 inputA_arr.append(curr_inputA)\n",
        "#                 inputB_arr.append(curr_inputB)\n",
        "#                 output_arr.append(curr_output)\n",
        "#                 i += self.shift\n",
        "#             if self.keep_smaller_window and i < len(curr_df) - 1:    ## i cannot be curr_df[-1:] of length 1, since impossible to split into input and output data\n",
        "#                 curr_chunk = curr_df.iloc[i:]\n",
        "#                 curr_inputA, curr_inputB, curr_output = self.single_kdi_input(curr_chunk)\n",
        "#                 inputA_arr.append(curr_inputA)\n",
        "#                 inputB_arr.append(curr_inputB)\n",
        "#                 output_arr.append(curr_output)\n",
        "#         return np.stack(inputA_arr, axis=0), np.stack(inputB_arr, axis=0), np.stack(output_arr, axis=0)\n",
        "    \n",
        "\n",
        "#     def generate_kdi(self):\n",
        "#         dataset = tf.data.Dataset.from_tensor_slices(({'inputA': self.inputA, 'inputB': self.inputB}, \n",
        "#                                                       self.output)).batch(self.batch_size)\n",
        "#         return dataset"
      ],
      "metadata": {
        "id": "OQ_Hx8r0i0xe"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Callbacks"
      ],
      "metadata": {
        "id": "z562ietSgYt_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## functionalize callbacks\n",
        "def create_checkpoint_callback(experiment_name, \n",
        "                               avg_mode,\n",
        "                               save_weights_only=True, \n",
        "                               monitor='val_loss', \n",
        "                               mode='min', \n",
        "                               save_best_only=True):\n",
        "    path = '/content/drive/MyDrive/COMP576/experiments'\n",
        "    now_time = datetime.datetime.now(timezone('America/Chicago'))\n",
        "    checkpoint_filepath = path + \"/\" + \"checkpoints\" + \"/\" + experiment_name + \"/\" + now_time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "    checkpoint_filepath = checkpoint_filepath + '-avg' if avg_mode else checkpoint_filepath\n",
        "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,\n",
        "                                                             save_weights_only=save_weights_only,\n",
        "                                                             monitor=monitor,\n",
        "                                                             mode=mode,\n",
        "                                                             save_best_only=save_best_only)\n",
        "    print(f\"Saving ModelCheckpoint files to :{checkpoint_filepath}\")\n",
        "    return checkpoint_callback\n",
        "\n",
        "def create_lr_scheduler(max_cap):\n",
        "    def lr_finder(epoch):\n",
        "        num1 = 4 - (epoch - 1) // 3\n",
        "        num2 = 1 + (epoch - 1) % 3 * 3\n",
        "        lr = round(0.1 ** num1 * num2, 7)\n",
        "        if max_cap and lr > max_cap:\n",
        "            return max_cap\n",
        "        return lr\n",
        "    return tf.keras.callbacks.LearningRateScheduler(lr_finder)\n",
        "\n",
        "def create_tensorboard_callback(experiment_name, avg_mode):\n",
        "    path = '/content/drive/MyDrive/COMP576/experiments'\n",
        "    now_time = datetime.datetime.now(timezone('America/Chicago'))\n",
        "    log_dir = path + \"/\" + \"tensorboard\" + \"/\" + experiment_name + \"/\" + now_time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "    log_dir = log_dir + '-avg' if avg_mode else log_dir\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
        "    print(f\"Saving TensorBoard log files to :{log_dir}\")\n",
        "    return tensorboard_callback\n",
        "\n",
        "def create_earlystopping_callback(patience, monitor='val_loss'):\n",
        "    return tf.keras.callbacks.EarlyStopping(monitor=monitor, patience=patience)\n",
        "\n",
        "def get_callbacks(experiment_name, patience, avg_mode):\n",
        "    earlystopping = create_earlystopping_callback(patience)\n",
        "    modelcheckpoint = create_checkpoint_callback(experiment_name=experiment_name, avg_mode=avg_mode)\n",
        "    tensorboard = create_tensorboard_callback(experiment_name=experiment_name, avg_mode=avg_mode)\n",
        "    return [earlystopping, modelcheckpoint, tensorboard]"
      ],
      "metadata": {
        "id": "kbDr2uyqgdIW"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline Model"
      ],
      "metadata": {
        "id": "EYlvFEjsgjVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def typenet_base(inputA):\n",
        "    name = 'TypeNet-base'\n",
        "    batch_1 = keras.layers.BatchNormalization()(inputA)\n",
        "    lstm_1 = keras.layers.LSTM(128, return_sequences=True)(batch_1)\n",
        "    dropout_1 = keras.layers.Dropout(0.5)(lstm_1)\n",
        "    batch_2 = keras.layers.BatchNormalization()(dropout_1)\n",
        "    lstm_2 = keras.layers.LSTM(128)(batch_2)\n",
        "    return name, lstm_2\n",
        "\n",
        "def typenet_gru_base(inputA, gru_units):\n",
        "    name = 'TypeNet-base'\n",
        "    batch_1 = keras.layers.BatchNormalization()(inputA)\n",
        "    gru_1 = keras.layers.GRU(gru_units, return_sequences=True)(batch_1)\n",
        "    dropout_1 = keras.layers.Dropout(0.5)(gru_1)\n",
        "    batch_2 = keras.layers.BatchNormalization()(dropout_1)\n",
        "    gru_2 = keras.layers.GRU(gru_units)(batch_2)\n",
        "    return name, gru_2\n",
        "\n",
        "def concate_RNN_base(concat, feature_dim, output_dim, gru_units, gru_units_2):\n",
        "    name = 'ConcatRNN-base'\n",
        "    reshape = keras.layers.Reshape((gru_units+feature_dim-output_dim, 1))(concat)\n",
        "    gru_1 = keras.layers.GRU(gru_units_2)(reshape)\n",
        "    return name, gru_1\n",
        "\n",
        "def create_model(feature_dim, output_dim, user_embedding=None, keycode_embedding=None, concat_model=None):\n",
        "    inputA = keras.layers.Input(shape=[None, feature_dim], name='InputA')\n",
        "    inputB = keras.layers.Input(shape=[feature_dim-output_dim], name='InputB')\n",
        "    \n",
        "    if user_embedding:\n",
        "        user_name, user_embeded = user_embedding(inputA)\n",
        "    else:\n",
        "        user_name, user_embeded = 'no-user', inputA\n",
        "    if keycode_embedding:\n",
        "        keycode_name, keycode_embeded = keycode_embedding(inputB)\n",
        "    else:\n",
        "        keycode_name, keycode_embeded = 'no-keycode', inputB\n",
        "    concat = keras.layers.concatenate([user_embeded, keycode_embeded])\n",
        "\n",
        "    if concat_model:\n",
        "        concat_name, concat_output = concat_model(concat, feature_dim, output_dim)\n",
        "    else:\n",
        "        concat_name, concat_output = 'no-concat', concat\n",
        "    output = keras.layers.Dense(output_dim)(concat_output)\n",
        "\n",
        "    model_name = user_name + '_' + keycode_name + '_' + concat_name\n",
        "    return keras.Model(inputs=[inputA, inputB], outputs=[output], name=model_name)"
      ],
      "metadata": {
        "id": "1_ycvrWhgkPj"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Miscellaneous Code"
      ],
      "metadata": {
        "id": "s59wveuJmxqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lr_vs_loss(history):\n",
        "    lrs = history.history['lr']\n",
        "    loss = history.history['val_loss']\n",
        "    plt.semilogx(lrs, loss);\n",
        "\n",
        "def preprocess_message(df, df_name):\n",
        "    print(f\"In total {len(df['PARTICIPANT_ID'].unique())} users in the {df_name} dataset, with {len(df)} keystroke samples\")"
      ],
      "metadata": {
        "id": "9wLaT8LVmzlU"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMPORT CHECK"
      ],
      "metadata": {
        "id": "ACrmWYG-Mnj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\\nHello from KDR_Preprocessing!\")"
      ],
      "metadata": {
        "id": "01xIrsyFMpXV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd515d9e-0957-4ceb-be98-8b7c65ab60f4"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Hello from KDR_Preprocessing!\n"
          ]
        }
      ]
    }
  ]
}